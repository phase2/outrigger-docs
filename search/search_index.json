{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Outrigger Easy containerized development environments based on Docker The various projects within the Outrigger ecosystem provide simple to manage project environments for any technology and deployment architecture. These environments are based upon Docker and Outrigger helps you tie the pieces together to provide a stable foundation for project work that includes: A Docker Machine based VM DNS services Network routing High speed filesystems Persistent data storage The Outrigger approach is to configure and use standard Docker tools. This allows you the option to learn these tools when you want to and provides the flexibility to override or substitute components should it be necessary for your project. If you are brand new to Outrigger and Docker, it is highly recommended you read the Glossary and Architecture documents to get familiar with the concepts and terminology used throughout this documentation.","title":"Home"},{"location":"#outrigger","text":"Easy containerized development environments based on Docker The various projects within the Outrigger ecosystem provide simple to manage project environments for any technology and deployment architecture. These environments are based upon Docker and Outrigger helps you tie the pieces together to provide a stable foundation for project work that includes: A Docker Machine based VM DNS services Network routing High speed filesystems Persistent data storage The Outrigger approach is to configure and use standard Docker tools. This allows you the option to learn these tools when you want to and provides the flexibility to override or substitute components should it be necessary for your project. If you are brand new to Outrigger and Docker, it is highly recommended you read the Glossary and Architecture documents to get familiar with the concepts and terminology used throughout this documentation.","title":"Outrigger"},{"location":"appendix/architecture/","text":"Architecture Outrigger uses a containerization based approach to providing a project environment. Project services running as containers are combined with configuration for persistent data storage, DNS services and network manipulation which allows for easy access via names like service.project.vm rather than IP addresses. The Outrigger CLI facilitates setting up a VM to act as a container host on systems which can not host containers natively. It also configures the VM with the following: Filesystem sharing for high performance file access of the host machine Container initiation for DNS services Network and name resolution configuration Layered with this core are a set of docker images which provide common project services such as a web server along with the ability to configure these services with commonly desired development options. Any docker image can be used if desired. Docker Compose is used to control containers to provide a complete environment and a set of Yeoman based generators are used to help initial project setup to ensure all pieces fit together using the current best known options.. Containerization In order to provide lightweight environments and tooling we use a technical approach called containers. Containers attempt to move the unit of environment from server to application. This allows separation of concerns between how an application is configured, how the containers communicate with each other, and where the containers are deployed. Take an advanced Drupal stack that includes Apache/PHP, MySQL, Memcache, and SOLR. With each component configured to run in its own container, the containers can all run on a single VM for local development and be spread across multiple servers for an optimized production deployment. Let us briefly touch on the technology we will be using and how it conceptually fits together. Docker The container implementation we use is called Docker which is explained in the intro: What is Docker ? This is the way we capture environment units for our application/services and share them with everyone on the team. Environments are captured as images, similar to a VM, so when anyone runs that image they all start with the exact same set of files. For example, nearly every project needs a web server so we have a container image that can be run to provide that service. Our Docker images are also set up to allow for configuration adjustment to enable common development options. rig This is a project that glues all of the hosting aspects of these tools together into an easy to use unit. You can find rig in a GitHub repository here . There are 2 basic services that rig provides: Manage virtual machines for running containers The rig binary will manage the creation/configuration/upgrade/start/stop of boot2docker virtual machines (a.k.a Docker Hosts) via docker-machine. It ensures that the docker-machine virtual machine is the right version, is named correctly and configured to run efficiently within VirtualBox, VMWare Fusion or xhyve. Nice DNS names and routing for running containers Once there is a safe environment to run our containers we need a way to route traffic to them and provide easy to use/remember domain names to make accessing these services simple. Domain names for containers are set in the docker-compose YAML files using configuration that the dnsdock container reads to create a mapping between domain name and container. We use dnsdock running as a container within the Docker Host. The dnsdock service, which listens on 172.17.0.1:53535. The dnsdock container resolves the *.vm domain names to the IP addresses of the containers. Internal container names will look like web.openatrium.vm . All Outrigger containers will carry the .vm extension for name resolution. There is additional information in DNS Resolution . Docker Hub Docker Hub is where container images are stored and retrieved when your local machine does not already have a copy of the requested container image. Docker Hub can be thought of like GitHub or BitBucket, and Docker Hub images can be thought of as git repositories. We can make new versions of the images and they can be pushed and pulled to the Docker Hub service. Container Configuration If a container wants to offer configurable options it will document how to control it within the README or via Environment variables in the Dockerfile itself. See our Apache / PHP Dockerfile for an example. In this Docker Image, passing environment variables can override the PHP memory limit. Those variables can either be passed on the command line when executing a docker run command directly, or in the environment section of a docker-compose file. Documentation entries within the Common Tasks section provide additional approaches to container configuration.","title":"Architecture"},{"location":"appendix/architecture/#architecture","text":"Outrigger uses a containerization based approach to providing a project environment. Project services running as containers are combined with configuration for persistent data storage, DNS services and network manipulation which allows for easy access via names like service.project.vm rather than IP addresses. The Outrigger CLI facilitates setting up a VM to act as a container host on systems which can not host containers natively. It also configures the VM with the following: Filesystem sharing for high performance file access of the host machine Container initiation for DNS services Network and name resolution configuration Layered with this core are a set of docker images which provide common project services such as a web server along with the ability to configure these services with commonly desired development options. Any docker image can be used if desired. Docker Compose is used to control containers to provide a complete environment and a set of Yeoman based generators are used to help initial project setup to ensure all pieces fit together using the current best known options..","title":"Architecture"},{"location":"appendix/architecture/#containerization","text":"In order to provide lightweight environments and tooling we use a technical approach called containers. Containers attempt to move the unit of environment from server to application. This allows separation of concerns between how an application is configured, how the containers communicate with each other, and where the containers are deployed. Take an advanced Drupal stack that includes Apache/PHP, MySQL, Memcache, and SOLR. With each component configured to run in its own container, the containers can all run on a single VM for local development and be spread across multiple servers for an optimized production deployment. Let us briefly touch on the technology we will be using and how it conceptually fits together.","title":"Containerization"},{"location":"appendix/architecture/#docker","text":"The container implementation we use is called Docker which is explained in the intro: What is Docker ? This is the way we capture environment units for our application/services and share them with everyone on the team. Environments are captured as images, similar to a VM, so when anyone runs that image they all start with the exact same set of files. For example, nearly every project needs a web server so we have a container image that can be run to provide that service. Our Docker images are also set up to allow for configuration adjustment to enable common development options.","title":"Docker"},{"location":"appendix/architecture/#rig","text":"This is a project that glues all of the hosting aspects of these tools together into an easy to use unit. You can find rig in a GitHub repository here . There are 2 basic services that rig provides:","title":"rig"},{"location":"appendix/architecture/#manage-virtual-machines-for-running-containers","text":"The rig binary will manage the creation/configuration/upgrade/start/stop of boot2docker virtual machines (a.k.a Docker Hosts) via docker-machine. It ensures that the docker-machine virtual machine is the right version, is named correctly and configured to run efficiently within VirtualBox, VMWare Fusion or xhyve.","title":"Manage virtual machines for running containers"},{"location":"appendix/architecture/#nice-dns-names-and-routing-for-running-containers","text":"Once there is a safe environment to run our containers we need a way to route traffic to them and provide easy to use/remember domain names to make accessing these services simple. Domain names for containers are set in the docker-compose YAML files using configuration that the dnsdock container reads to create a mapping between domain name and container. We use dnsdock running as a container within the Docker Host. The dnsdock service, which listens on 172.17.0.1:53535. The dnsdock container resolves the *.vm domain names to the IP addresses of the containers. Internal container names will look like web.openatrium.vm . All Outrigger containers will carry the .vm extension for name resolution. There is additional information in DNS Resolution .","title":"Nice DNS names and routing for running containers"},{"location":"appendix/architecture/#docker-hub","text":"Docker Hub is where container images are stored and retrieved when your local machine does not already have a copy of the requested container image. Docker Hub can be thought of like GitHub or BitBucket, and Docker Hub images can be thought of as git repositories. We can make new versions of the images and they can be pushed and pulled to the Docker Hub service.","title":"Docker Hub"},{"location":"appendix/architecture/#container-configuration","text":"If a container wants to offer configurable options it will document how to control it within the README or via Environment variables in the Dockerfile itself. See our Apache / PHP Dockerfile for an example. In this Docker Image, passing environment variables can override the PHP memory limit. Those variables can either be passed on the command line when executing a docker run command directly, or in the environment section of a docker-compose file. Documentation entries within the Common Tasks section provide additional approaches to container configuration.","title":"Container Configuration"},{"location":"appendix/background/","text":"Background This document covers some of the history of development tooling and explains some of the how and why of the Outrigger approach. Why Outrigger Outrigger covers an entire toolbox of solutions to accelerate development and smoothly implement best practices. We intend to use the Outrigger umbrella to help us roll these tools and approaches out to the whole company in a more consistent fashion. Across a wide variety of projects and clients, keeping a development, integration, staging, production and other environments synchronized in terms of server configuration and supporting software versions and tooling has always been a challenge. The multitude of platforms and versions of operating systems and software is only growing. In the past we have used Vagrant in conjunction with a virtual machine (VM) for local development environments, project-based VMs on OpenStack for integration environments and tools like Puppet to try and ensure all of those match each other and the deployment environments. It\u2019s been better than doing it all by hand but it has required building additional expertise above typical system management tools and it\u2019s relatively inefficient to have many project-specific VMs locally. On top of that are the various, sometimes conflicting, versions of development toolchains which the VMs don\u2019t always address. Outrigger is going to address this problem by providing an efficient way to have project/app-specific environments as well as development tooling by using a concept called containerization. This lets us focus on items at a service-and-tools level rather than at a server level and make the details of our hosting implementation more transparent to all technical team members. Mapping Concepts From Previous Approaches There are some important pieces to remember about how containerization is similar and different to concepts you may be familiar with. Vagrant Vagrant is being replaced by the combination of Docker Compose and Docker Machine. Virtual Machines For local development, you should no longer worry about Virtual Machines at a project level as they are replaced by a single Docker Machine instance. In environments where containers are meant to run on a cluster of hosts, orchestration tools like Kubernetes or Swarm may be used to distribute and coordinate containers across multiple Docker Hosts. Puppet / Ansible / configuration management Goodbye, for local environments at least. Docker uses a conceptually different approach to configuration management than Puppet. It\u2019s perfectly possible to use tools like Puppet and Ansible from within a Dockerfile to get a container image prepared though typically simple shell commands are preferred. The idea is that systems like Puppet are no longer needed to manage upgrades across working servers/containers. When there are updates needed you will update the image, pull down the new version of the image to your server and then stop the containers running the old version of the image and start containers based on the new version of the image.","title":"Background"},{"location":"appendix/background/#background","text":"This document covers some of the history of development tooling and explains some of the how and why of the Outrigger approach.","title":"Background"},{"location":"appendix/background/#why-outrigger","text":"Outrigger covers an entire toolbox of solutions to accelerate development and smoothly implement best practices. We intend to use the Outrigger umbrella to help us roll these tools and approaches out to the whole company in a more consistent fashion. Across a wide variety of projects and clients, keeping a development, integration, staging, production and other environments synchronized in terms of server configuration and supporting software versions and tooling has always been a challenge. The multitude of platforms and versions of operating systems and software is only growing. In the past we have used Vagrant in conjunction with a virtual machine (VM) for local development environments, project-based VMs on OpenStack for integration environments and tools like Puppet to try and ensure all of those match each other and the deployment environments. It\u2019s been better than doing it all by hand but it has required building additional expertise above typical system management tools and it\u2019s relatively inefficient to have many project-specific VMs locally. On top of that are the various, sometimes conflicting, versions of development toolchains which the VMs don\u2019t always address. Outrigger is going to address this problem by providing an efficient way to have project/app-specific environments as well as development tooling by using a concept called containerization. This lets us focus on items at a service-and-tools level rather than at a server level and make the details of our hosting implementation more transparent to all technical team members.","title":"Why Outrigger"},{"location":"appendix/background/#mapping-concepts-from-previous-approaches","text":"There are some important pieces to remember about how containerization is similar and different to concepts you may be familiar with.","title":"Mapping Concepts From Previous Approaches"},{"location":"appendix/background/#vagrant","text":"Vagrant is being replaced by the combination of Docker Compose and Docker Machine.","title":"Vagrant"},{"location":"appendix/background/#virtual-machines","text":"For local development, you should no longer worry about Virtual Machines at a project level as they are replaced by a single Docker Machine instance. In environments where containers are meant to run on a cluster of hosts, orchestration tools like Kubernetes or Swarm may be used to distribute and coordinate containers across multiple Docker Hosts.","title":"Virtual Machines"},{"location":"appendix/background/#puppet-ansible-configuration-management","text":"Goodbye, for local environments at least. Docker uses a conceptually different approach to configuration management than Puppet. It\u2019s perfectly possible to use tools like Puppet and Ansible from within a Dockerfile to get a container image prepared though typically simple shell commands are preferred. The idea is that systems like Puppet are no longer needed to manage upgrades across working servers/containers. When there are updates needed you will update the image, pull down the new version of the image to your server and then stop the containers running the old version of the image and start containers based on the new version of the image.","title":"Puppet / Ansible / configuration management"},{"location":"appendix/glossary/","text":"Glossary Host Machine Your laptop for the purposes of Outrigger. This is where your project's source code, your IDE, browsers, etc run as well as where the virtual machine acting as the Docker Host runs. Docker Host This is a Linux-based virtual machine capable of running Docker. One Docker host VM can run multiple containers for multiple projects. Docker Image A read only template that can be instantiated as a running container. An image might supply a service as small as a build tool or as large as a database and/or web server. Images are generally single purpose in nature and are then linked together to build more complex capabilities. Container A runtime instance of a Docker Image. This is what contains a service like Apache or MySQL. Several containers can run on a single Docker Host and they can be linked so that they they know how to communicate with each other. Docker Machine Docker Machine creates the Docker Host virtual machine in which containers run. This provides the functionality that we previously used Vagrant to accomplish. Under the covers docker-machine starts up a virtual machine running a tiny version of Linux. On Macs this tiny version of Linux is called boot2docker. All of the Docker commands to build and run containers will actually be executed on the boot2docker virtual machine. On your laptop you will have a single boot2docker virtual machine (running in VirtualBox or VMWare Fusion) and it will host one or more Docker containers. Each project will likely have multiple Docker containers running (web server, database, cache, search, etc.). Docker Compose Docker Compose is used to manage and coordinate the containers that need to run for a project in an easy to use YAML file. Each project will have a docker-compose file for each Docker environment that the project supports. For example, the default docker-compose.yml would be used to start containers for local development, and alternate Docker compose files could be included for starting containers for the integration/stage and other environments.","title":"Glossary"},{"location":"appendix/glossary/#glossary","text":"","title":"Glossary"},{"location":"appendix/glossary/#host-machine","text":"Your laptop for the purposes of Outrigger. This is where your project's source code, your IDE, browsers, etc run as well as where the virtual machine acting as the Docker Host runs.","title":"Host Machine"},{"location":"appendix/glossary/#docker-host","text":"This is a Linux-based virtual machine capable of running Docker. One Docker host VM can run multiple containers for multiple projects.","title":"Docker Host"},{"location":"appendix/glossary/#docker-image","text":"A read only template that can be instantiated as a running container. An image might supply a service as small as a build tool or as large as a database and/or web server. Images are generally single purpose in nature and are then linked together to build more complex capabilities.","title":"Docker Image"},{"location":"appendix/glossary/#container","text":"A runtime instance of a Docker Image. This is what contains a service like Apache or MySQL. Several containers can run on a single Docker Host and they can be linked so that they they know how to communicate with each other.","title":"Container"},{"location":"appendix/glossary/#docker-machine","text":"Docker Machine creates the Docker Host virtual machine in which containers run. This provides the functionality that we previously used Vagrant to accomplish. Under the covers docker-machine starts up a virtual machine running a tiny version of Linux. On Macs this tiny version of Linux is called boot2docker. All of the Docker commands to build and run containers will actually be executed on the boot2docker virtual machine. On your laptop you will have a single boot2docker virtual machine (running in VirtualBox or VMWare Fusion) and it will host one or more Docker containers. Each project will likely have multiple Docker containers running (web server, database, cache, search, etc.).","title":"Docker Machine"},{"location":"appendix/glossary/#docker-compose","text":"Docker Compose is used to manage and coordinate the containers that need to run for a project in an easy to use YAML file. Each project will have a docker-compose file for each Docker environment that the project supports. For example, the default docker-compose.yml would be used to start containers for local development, and alternate Docker compose files could be included for starting containers for the integration/stage and other environments.","title":"Docker Compose"},{"location":"appendix/library/","text":"Library This library is meant to be a listing of resources from the Interwebz to help you understand how to use Outrigger and various related technologies.","title":"Library"},{"location":"appendix/library/#library","text":"This library is meant to be a listing of resources from the Interwebz to help you understand how to use Outrigger and various related technologies.","title":"Library"},{"location":"appendix/return-codes/","text":"Return Codes Return codes, sometimes called exit codes, are numbers returned from the rig executable to indicate the status of the requested command at exit. These codes can be interpreted by scripts calling rig to react to success or failure states. Generally a return code of 0 is used to indicate success. Non-zero return codes communicate that an error has occurred and give an idea of the error type through the value returned. rig exit code values are: 1 - Default Returned when an uncaught error is raised and the program exited unexpectedly. This is used by the underlying cli framework as well. Project scripts may also return this code. 12 - Environmental Returned when there is a problem with the execution environment and the current command cannot continue. This is usually the result of the Docker Machine not running or when Docker is not configured correctly. Backup and restore errors generally fall in this category too. 13 - External Command / Upstream Returned when external scripts or setup commands have reported failure.","title":"Return Codes"},{"location":"appendix/return-codes/#return-codes","text":"Return codes, sometimes called exit codes, are numbers returned from the rig executable to indicate the status of the requested command at exit. These codes can be interpreted by scripts calling rig to react to success or failure states. Generally a return code of 0 is used to indicate success. Non-zero return codes communicate that an error has occurred and give an idea of the error type through the value returned. rig exit code values are: 1 - Default Returned when an uncaught error is raised and the program exited unexpectedly. This is used by the underlying cli framework as well. Project scripts may also return this code. 12 - Environmental Returned when there is a problem with the execution environment and the current command cannot continue. This is usually the result of the Docker Machine not running or when Docker is not configured correctly. Backup and restore errors generally fall in this category too. 13 - External Command / Upstream Returned when external scripts or setup commands have reported failure.","title":"Return Codes"},{"location":"common-tasks/accessing-logs/","text":"Accessing Logs Viewing logs from my container services When you start a your containers via docker-compose up all of the defined services will start in the foreground. All log output to stdout/stderr within each container will be output the console. Each entry will be prefixed with the name of the running container to identify the source of the log message. If log output is not coming directly to the console (for example, if you started your containers via docker-compose up -d to detach them) you can still SSH into the container and browse the file system for logs. Most common services will provide some log output in /var/log . Alternatively, another option is to run the docker logs containername command.","title":"Accessing logs"},{"location":"common-tasks/accessing-logs/#accessing-logs","text":"","title":"Accessing Logs"},{"location":"common-tasks/accessing-logs/#viewing-logs-from-my-container-services","text":"When you start a your containers via docker-compose up all of the defined services will start in the foreground. All log output to stdout/stderr within each container will be output the console. Each entry will be prefixed with the name of the running container to identify the source of the log message. If log output is not coming directly to the console (for example, if you started your containers via docker-compose up -d to detach them) you can still SSH into the container and browse the file system for logs. Most common services will provide some log output in /var/log . Alternatively, another option is to run the docker logs containername command.","title":"Viewing logs from my container services"},{"location":"common-tasks/creating-your-own-images/","text":"Creating your own images Creating a Dockerfile The Dockerfile you create for your project should represent the application as it will need to run in production. There are many reasons to have a Dockerfile to create an image, but if you don't intend on running containers in production you can likely skip this part. The most minimal project container will need to have your code in it. Start by determining which Docker image you will need to extend. For the general Drupal use cases you will base your project Dockerfile on apache-php:php70. FROM phase2/apache-php:php70 Then copy your code into the correct place for the container. For a PHP/Drupal project, copy the materialized site code into the container docroot COPY ./html /var/www/html/ Build the Dockerfile into an image docker build -t some-name . On successful build, test the image by running docker run -t some-name Here is the full (simple) Dockerfile FROM phase2/apache-php:php70 # Copy in the site COPY ./html /var/www/html/","title":"Creating your own images"},{"location":"common-tasks/creating-your-own-images/#creating-your-own-images","text":"","title":"Creating your own images"},{"location":"common-tasks/creating-your-own-images/#creating-a-dockerfile","text":"The Dockerfile you create for your project should represent the application as it will need to run in production. There are many reasons to have a Dockerfile to create an image, but if you don't intend on running containers in production you can likely skip this part. The most minimal project container will need to have your code in it. Start by determining which Docker image you will need to extend. For the general Drupal use cases you will base your project Dockerfile on apache-php:php70. FROM phase2/apache-php:php70 Then copy your code into the correct place for the container. For a PHP/Drupal project, copy the materialized site code into the container docroot COPY ./html /var/www/html/ Build the Dockerfile into an image docker build -t some-name . On successful build, test the image by running docker run -t some-name Here is the full (simple) Dockerfile FROM phase2/apache-php:php70 # Copy in the site COPY ./html /var/www/html/","title":"Creating a Dockerfile"},{"location":"common-tasks/customizing-container-configuration/","text":"Customizing Configuration There are a few primary mechanisms for customizing the configuration of a container. Environment Variables Many images will react to environmental variables to alter their behavior. Images which offer this functionality should document it in the README for the image. For an example see the Outrigger Build image. When environmental configuration is available it can be triggered by specifying the appropriate value in the environments section of your docker compose file. www: image: phase2/apache-php:php56 environment: # Change some core container settings PHP_MAX_EXECUTION_TIME: 45 # These enable debug/profiling support PHP_XDEBUG: true PHP_XHPROF: false Volume Mounts If the container image you are using doesn't allow for change via environmental variables, your next option is to override the configuration file using volume mounting. In this setup, you replace the configuration file inside a container with a file from your host machine. www: image: phase2/apache-php:php56 volumes: # substitute in a special mime magic file because our project handles files # of special types - ./env/www/etc/httpd/conf/magic:/etc/httpd/conf/magic Volume Mounts for confd Phase2 images often use confd to template configuration from environment variable and other sources. The configuration file you may be trying to override might be generated on container initiation via confd. If this is the case, you'll need to override the template file from which the configuration file is created. If you find that your copy of the configuration file on your host machine is updated when you start a container this is likely the cause. For example, the Xdebug configuration provided in our phase2/apache-php:php70 container is provided by confd, so this is how you would override that configuration www: image: phase2/apache-php:php70 volumes: # substitute a different confd template file into the image so confd will use the override on container boot - ./env/www/etc/confd/templates/xdebug.ini.tmpl:/etc/confd/templates/xdebug.ini.tmpl","title":"Changing container configuration"},{"location":"common-tasks/customizing-container-configuration/#customizing-configuration","text":"There are a few primary mechanisms for customizing the configuration of a container.","title":"Customizing Configuration"},{"location":"common-tasks/customizing-container-configuration/#environment-variables","text":"Many images will react to environmental variables to alter their behavior. Images which offer this functionality should document it in the README for the image. For an example see the Outrigger Build image. When environmental configuration is available it can be triggered by specifying the appropriate value in the environments section of your docker compose file. www: image: phase2/apache-php:php56 environment: # Change some core container settings PHP_MAX_EXECUTION_TIME: 45 # These enable debug/profiling support PHP_XDEBUG: true PHP_XHPROF: false","title":"Environment Variables"},{"location":"common-tasks/customizing-container-configuration/#volume-mounts","text":"If the container image you are using doesn't allow for change via environmental variables, your next option is to override the configuration file using volume mounting. In this setup, you replace the configuration file inside a container with a file from your host machine. www: image: phase2/apache-php:php56 volumes: # substitute in a special mime magic file because our project handles files # of special types - ./env/www/etc/httpd/conf/magic:/etc/httpd/conf/magic","title":"Volume Mounts"},{"location":"common-tasks/customizing-container-configuration/#volume-mounts-for-confd","text":"Phase2 images often use confd to template configuration from environment variable and other sources. The configuration file you may be trying to override might be generated on container initiation via confd. If this is the case, you'll need to override the template file from which the configuration file is created. If you find that your copy of the configuration file on your host machine is updated when you start a container this is likely the cause. For example, the Xdebug configuration provided in our phase2/apache-php:php70 container is provided by confd, so this is how you would override that configuration www: image: phase2/apache-php:php70 volumes: # substitute a different confd template file into the image so confd will use the override on container boot - ./env/www/etc/confd/templates/xdebug.ini.tmpl:/etc/confd/templates/xdebug.ini.tmpl","title":"Volume Mounts for confd"},{"location":"common-tasks/dns-resolution/","text":"DNS Resolution DNS Names for your containers Within the labels section of the docker-compose file you can specify labels that will control the DNS name of your containers. DNS names are in the format of [name].[image].vm (e.g web.outrigger.vm) com.dnsdock.name - This is the type of container. Usually something like web, db, cache, etc. com.dnsdock.image - This is generally your project name (e.g outrigger, drupal, etc.) Note All DNS names of Outrigger containers will end in .vm If you need multiple domains mapped to a container you can use the com.dnsdock.alias setting. It takes a full domain name and can take a comma separated list (e.g. otherhost.outrigger.vm or alexandria.phase2.vm). Additionally, dnsdock supports longer domain queries meaning if you have a service named web.outrigger.vm, you can also use something.web.outrigger.vm and it will resolve to the same IP address. For all other DNS configuration options, please see the dnsdock label documentation DNS Forwarders Outrigger can be configured to use additional name servers to forward DNS requests if the record cannot be resolved by dnsdock. An example is if you connect to a VPN and need to resolve addresses to private servers within a VPN. To enable this you need to configure the RIG_NAMESERVERS environment variable to a comma separated list of ip:port (example: 10.10.7.2:53,8.8.8.8:53 ) before running either rig start or rig dns . We suggest putting this env var configuration in your ~/.bashrc or ~/.zshrc so that it is always present when needed. If you just need this temporarily, you can pass the configuration in with the --nameservers command line option to rig start or rig dns . This configuration will try each forwarder name server, in order, to resolve names until success or all name servers have been exhausted. DNS Command There is also a rig dns command that will launch and configure our DNS services on any Docker Host. If you want to configure DNS on a Docker Host other than dev be sure to specify the --name parameter on rig . DNS Debugging If you want to see what containers have registered names, use the rig dns-records command. This command will list all registered container names and aliases along with the container's IP address.","title":"DNS Resolution"},{"location":"common-tasks/dns-resolution/#dns-resolution","text":"","title":"DNS Resolution"},{"location":"common-tasks/dns-resolution/#dns-names-for-your-containers","text":"Within the labels section of the docker-compose file you can specify labels that will control the DNS name of your containers. DNS names are in the format of [name].[image].vm (e.g web.outrigger.vm) com.dnsdock.name - This is the type of container. Usually something like web, db, cache, etc. com.dnsdock.image - This is generally your project name (e.g outrigger, drupal, etc.) Note All DNS names of Outrigger containers will end in .vm If you need multiple domains mapped to a container you can use the com.dnsdock.alias setting. It takes a full domain name and can take a comma separated list (e.g. otherhost.outrigger.vm or alexandria.phase2.vm). Additionally, dnsdock supports longer domain queries meaning if you have a service named web.outrigger.vm, you can also use something.web.outrigger.vm and it will resolve to the same IP address. For all other DNS configuration options, please see the dnsdock label documentation","title":"DNS Names for your containers"},{"location":"common-tasks/dns-resolution/#dns-forwarders","text":"Outrigger can be configured to use additional name servers to forward DNS requests if the record cannot be resolved by dnsdock. An example is if you connect to a VPN and need to resolve addresses to private servers within a VPN. To enable this you need to configure the RIG_NAMESERVERS environment variable to a comma separated list of ip:port (example: 10.10.7.2:53,8.8.8.8:53 ) before running either rig start or rig dns . We suggest putting this env var configuration in your ~/.bashrc or ~/.zshrc so that it is always present when needed. If you just need this temporarily, you can pass the configuration in with the --nameservers command line option to rig start or rig dns . This configuration will try each forwarder name server, in order, to resolve names until success or all name servers have been exhausted.","title":"DNS Forwarders"},{"location":"common-tasks/dns-resolution/#dns-command","text":"There is also a rig dns command that will launch and configure our DNS services on any Docker Host. If you want to configure DNS on a Docker Host other than dev be sure to specify the --name parameter on rig .","title":"DNS Command"},{"location":"common-tasks/dns-resolution/#dns-debugging","text":"If you want to see what containers have registered names, use the rig dns-records command. This command will list all registered container names and aliases along with the container's IP address.","title":"DNS Debugging"},{"location":"common-tasks/inter-vm-communication/","text":"Accessing containers from other Virtual Machines If you use virtual machines to perform cross browser testing you'll need to know how to configure VirtualBox networking to allow for communication with your containers. Networking Mode In your virtual machine settings, under Network choose an enabled adapter and then choose NAT for the Attached to option. This will ensure that processes from within virtual machine can route requests through the network to your containers. DNS In addition to the network setup, you'll need to choose one of the following options so that the DNS entries for your containers can resolve to the proper IP address. Outrigger DNS The preferred option is to configure your virtual machine so that it uses the DNS services set up by Outrigger. To do so, set 172.17.0.1 as a DNS server in the virtual machine. For Windows, you can use these instructions for using Google's Public DNS to navigate to the correct area for setting your DNS server. Use 172.17.0.1 instead of the Google IP addresses. For Linux systems, refer to the Linux DNS configuration options section of the Linux Installation instructions and choose the option that best works for your system. Hosts File If configuring DNS isn't a viable option for your virtual machine, a backup approach is to manually manage the mapping of container domain names to the appropriate IP address. To do this, any time you start or stop containers you will need to take the output of rig dns-records and update the hosts file in the virtual machine.","title":"Accessing containers from other VMs"},{"location":"common-tasks/inter-vm-communication/#accessing-containers-from-other-virtual-machines","text":"If you use virtual machines to perform cross browser testing you'll need to know how to configure VirtualBox networking to allow for communication with your containers.","title":"Accessing containers from other Virtual Machines"},{"location":"common-tasks/inter-vm-communication/#networking-mode","text":"In your virtual machine settings, under Network choose an enabled adapter and then choose NAT for the Attached to option. This will ensure that processes from within virtual machine can route requests through the network to your containers.","title":"Networking Mode"},{"location":"common-tasks/inter-vm-communication/#dns","text":"In addition to the network setup, you'll need to choose one of the following options so that the DNS entries for your containers can resolve to the proper IP address.","title":"DNS"},{"location":"common-tasks/inter-vm-communication/#outrigger-dns","text":"The preferred option is to configure your virtual machine so that it uses the DNS services set up by Outrigger. To do so, set 172.17.0.1 as a DNS server in the virtual machine. For Windows, you can use these instructions for using Google's Public DNS to navigate to the correct area for setting your DNS server. Use 172.17.0.1 instead of the Google IP addresses. For Linux systems, refer to the Linux DNS configuration options section of the Linux Installation instructions and choose the option that best works for your system.","title":"Outrigger DNS"},{"location":"common-tasks/inter-vm-communication/#hosts-file","text":"If configuring DNS isn't a viable option for your virtual machine, a backup approach is to manually manage the mapping of container domain names to the appropriate IP address. To do this, any time you start or stop containers you will need to take the output of rig dns-records and update the hosts file in the virtual machine.","title":"Hosts File"},{"location":"common-tasks/multiple-docker-machines/","text":"Multiple Docker Machines There are special cases in which you may want to have multiple Docker Hosts running, or use a Docker Machine from another client/project. One example of this might be to test the performance differences between VirtualBox and xhyve. For these cases, rig takes a --name flag that will allow you to specify name of the Docker Host. For all commands this name defaults to dev but you can configure this name if needed. Also, if you don't want to specify the Docker Host name on every command you can specify the RIG_ACTIVE_MACHINE environment variable to be Docker Machine name of the VM you care to interact with. This way you can set it once and forget about it. Unless otherwise specified, the name defaults to dev for all commands.","title":"Multiple Docker Machines"},{"location":"common-tasks/multiple-docker-machines/#multiple-docker-machines","text":"There are special cases in which you may want to have multiple Docker Hosts running, or use a Docker Machine from another client/project. One example of this might be to test the performance differences between VirtualBox and xhyve. For these cases, rig takes a --name flag that will allow you to specify name of the Docker Host. For all commands this name defaults to dev but you can configure this name if needed. Also, if you don't want to specify the Docker Host name on every command you can specify the RIG_ACTIVE_MACHINE environment variable to be Docker Machine name of the VM you care to interact with. This way you can set it once and forget about it. Unless otherwise specified, the name defaults to dev for all commands.","title":"Multiple Docker Machines"},{"location":"common-tasks/routine-image-maintenance/","text":"Routine Image Maintenance Restart rig on a regular basis to minimize the risk of filesystem or other performance problems with your containers. Docker Image Updates Periodically update your Docker Images to ensure you have the correct configuration. This should be done in coordination with members of your project team so you are all working from the same version of image. Ideally all images in compose files are working off of tags to ensure consistency. Images should be pulled frequently though especially for images that are tagged with a language version, like php70 . Updates may go into those images and keep the same tag, so pulling regularly (and coordinating) will ensure you are getting state of the art. If your project uses \"loosely versioned\" Docker images (such as specifying latest as an image version tag or no tag at all which will default to latest ) your local update schedule should be coordinated with updates to other environments and team members. Here is how to update your images # Stop your containers in case they are running. docker-compose stop # Pull the latest changes for all images referenced in docker-compose.yml docker-compose pull # Pull the latest changes for all images referenced in build.yml. docker-compose -f build.yml pull # Remove your old containers to make sure you're using the latest images. docker-compose rm # Re-create your containers based on the freshly updated images. docker-compose up -d Updates for a Specified Image docker pull outrigger/build:php70 If either form of pull command fails, try re-running with the --no-cache option.","title":"Routine Image Maintenance"},{"location":"common-tasks/routine-image-maintenance/#routine-image-maintenance","text":"Restart rig on a regular basis to minimize the risk of filesystem or other performance problems with your containers.","title":"Routine Image Maintenance"},{"location":"common-tasks/routine-image-maintenance/#docker-image-updates","text":"Periodically update your Docker Images to ensure you have the correct configuration. This should be done in coordination with members of your project team so you are all working from the same version of image. Ideally all images in compose files are working off of tags to ensure consistency. Images should be pulled frequently though especially for images that are tagged with a language version, like php70 . Updates may go into those images and keep the same tag, so pulling regularly (and coordinating) will ensure you are getting state of the art. If your project uses \"loosely versioned\" Docker images (such as specifying latest as an image version tag or no tag at all which will default to latest ) your local update schedule should be coordinated with updates to other environments and team members. Here is how to update your images # Stop your containers in case they are running. docker-compose stop # Pull the latest changes for all images referenced in docker-compose.yml docker-compose pull # Pull the latest changes for all images referenced in build.yml. docker-compose -f build.yml pull # Remove your old containers to make sure you're using the latest images. docker-compose rm # Re-create your containers based on the freshly updated images. docker-compose up -d","title":"Docker Image Updates"},{"location":"common-tasks/routine-image-maintenance/#updates-for-a-specified-image","text":"docker pull outrigger/build:php70 If either form of pull command fails, try re-running with the --no-cache option.","title":"Updates for a Specified Image"},{"location":"common-tasks/setting-up-mail/","text":"Setting Up Mail Email is something that many projects need, but during development you likely do not want to actually send email, but you'd rather have sent mail captured for examination and released to a real mail server only in certain situations. To handle these and other situations we recommend Mailhog Using MailHog MailHog can be added as another service with your projects Docker Compose file. See the mail service defined in the Outrigger Example Mail Project that service can be copied into your projects docker-compose.yml file, or kept separate and started as needed. Using the configuration from the example you would configure your application to use mail.outrigger.vm:1025 as your SMTP server and you could view captured mail (and choose to release it) via the web interface accessible at http://mail.outrigger.vm:8025","title":"Setting up mail"},{"location":"common-tasks/setting-up-mail/#setting-up-mail","text":"Email is something that many projects need, but during development you likely do not want to actually send email, but you'd rather have sent mail captured for examination and released to a real mail server only in certain situations. To handle these and other situations we recommend Mailhog","title":"Setting Up Mail"},{"location":"common-tasks/setting-up-mail/#using-mailhog","text":"MailHog can be added as another service with your projects Docker Compose file. See the mail service defined in the Outrigger Example Mail Project that service can be copied into your projects docker-compose.yml file, or kept separate and started as needed. Using the configuration from the example you would configure your application to use mail.outrigger.vm:1025 as your SMTP server and you could view captured mail (and choose to release it) via the web interface accessible at http://mail.outrigger.vm:8025","title":"Using MailHog"},{"location":"common-tasks/ssh-into-a-container/","text":"SSH into a Container How do I SSH into a running container There is a docker exec command that can be used to connect to a container that is already running. Use docker ps to get the name of the existing container Use the command docker exec -it container name /bin/bash to get a bash shell in the container Generically, use docker exec -it container name command to execute whatever command you specify in the container. How do I run a command in my container The proper way to run a command in a container is: docker-compose run container name command . For example, to get a shell into your web container you might run docker-compose run web /bin/bash To run a series of commands, you must wrap them in a single command using a shell. For example: docker-compose run [name in yml] sh -c '[command 1] [command 2] [command 3]' In some cases you may want to run a container that is not defined by a docker-compose.yml file, for example to test a new container configuration. Use docker run to start a new container with a given image: docker run -it image name command The docker run command accepts command line options to specify volume mounts, environment variables, the working directory, and more. Getting a shell for build/tooling operations Getting a shell into a build container to execute any operations is the simplest approach. You simply want to get access to the cli container we defined in the compose file. The command docker-compose -f build.yml run cli will start an instance of the outrigger/build image and run a bash shell for you. From there you are free to use drush , grunt or whatever your little heart desires. Running commands, but not from a dedicated shell Another concept in the Docker world is starting a container to run a single command and allowing the container stop when the command is completed. This is great if you run commands infrequently, or don't want to have another container constantly running. Running your commands on containers in this fashion is also well suited for commands that don't generate any files on the filesystem or if they do, they write those files on to volumes mounted into the container. The drush container defined in the example build.yml file is a container designed specifically to run Drush in a single working directory taking only the commands as arguments. This approach allows us to provide a quick and easy mechanism for running any Drush command, such as sqlc , cache-rebuild , and others, in your Drupal site quick and easily. There are also other examples of a grunt command container similar to drush and an even more specific command container around running a single command, drush make to build the site from a make/dependency file.","title":"SSH into a container"},{"location":"common-tasks/ssh-into-a-container/#ssh-into-a-container","text":"","title":"SSH into a Container"},{"location":"common-tasks/ssh-into-a-container/#how-do-i-ssh-into-a-running-container","text":"There is a docker exec command that can be used to connect to a container that is already running. Use docker ps to get the name of the existing container Use the command docker exec -it container name /bin/bash to get a bash shell in the container Generically, use docker exec -it container name command to execute whatever command you specify in the container.","title":"How do I SSH into a running container"},{"location":"common-tasks/ssh-into-a-container/#how-do-i-run-a-command-in-my-container","text":"The proper way to run a command in a container is: docker-compose run container name command . For example, to get a shell into your web container you might run docker-compose run web /bin/bash To run a series of commands, you must wrap them in a single command using a shell. For example: docker-compose run [name in yml] sh -c '[command 1] [command 2] [command 3]' In some cases you may want to run a container that is not defined by a docker-compose.yml file, for example to test a new container configuration. Use docker run to start a new container with a given image: docker run -it image name command The docker run command accepts command line options to specify volume mounts, environment variables, the working directory, and more.","title":"How do I run a command in my container"},{"location":"common-tasks/ssh-into-a-container/#getting-a-shell-for-buildtooling-operations","text":"Getting a shell into a build container to execute any operations is the simplest approach. You simply want to get access to the cli container we defined in the compose file. The command docker-compose -f build.yml run cli will start an instance of the outrigger/build image and run a bash shell for you. From there you are free to use drush , grunt or whatever your little heart desires.","title":"Getting a shell for build/tooling operations"},{"location":"common-tasks/ssh-into-a-container/#running-commands-but-not-from-a-dedicated-shell","text":"Another concept in the Docker world is starting a container to run a single command and allowing the container stop when the command is completed. This is great if you run commands infrequently, or don't want to have another container constantly running. Running your commands on containers in this fashion is also well suited for commands that don't generate any files on the filesystem or if they do, they write those files on to volumes mounted into the container. The drush container defined in the example build.yml file is a container designed specifically to run Drush in a single working directory taking only the commands as arguments. This approach allows us to provide a quick and easy mechanism for running any Drush command, such as sqlc , cache-rebuild , and others, in your Drupal site quick and easily. There are also other examples of a grunt command container similar to drush and an even more specific command container around running a single command, drush make to build the site from a make/dependency file.","title":"Running commands, but not from a dedicated shell"},{"location":"common-tasks/ssh-keys-for-private-repos/","text":"SSH Keys for Private Repositories Forward/Import your SSH Key into the build container to clone private repositories Certain containers like Jenkins and build may need your private key. This is supported by importing your private key into the container via a volume mount. To get your private key into the build container, volume mount your key into the container at /root/.ssh/outrigger.key and it will be processed accordingly. ~/.ssh/id_rsa:/root/.ssh/outrigger.key Passphrase Protected Keys For interactive operations, such as use of the build container, passphrase protected keys do not pose a challenge as you can enter the passphrase when prompted. Services like Jenkins, however, can prove problematic. A solution to this is to use a container running an SSH agent with your key added and unlocked. Your container will be able to connect with the SSH agent via the use of a shared volume and the SSH_AUTH_SOCK variable that points to it. SSH Agent Container The following is an docker compose file, ssh.yml, with services defined to help run the necessary container for the agent and add your desired keys. It is set up for a single SSH agent to share with all of your projects. If you prefer project specific SSH agents, remove the external: true declaration in all sample files. In order to have a single agent for all projects, first create a named volume using docker volume create --name=ssh . This only needs to be done once and you'll be prompted to do so if you forget. # ssh.yml # Provides a service for the ssh-agent and a service for adding keys to it # # Agent will need to be restarted and have key(s) re-added to it any time your # docker host is restarted. # version: '3.2' services: # use via docker-compose -f ssh.yml up [-d] ssh-agent ssh-agent: container_name: ssh-agent image: whilp/ssh-agent:latest volumes: - ssh:/ssh # use via docker-compose -f ssh.yml run --rm ssh-add $HOME/.ssh/name_of_desired_key ssh-add: image: whilp/ssh-agent:latest entrypoint: [ ssh-add ] volumes: - ssh:/ssh - $HOME:$HOME volumes: ssh: external: true Connecting to the SSH Agent from other containers Any container you want to connect to the SSH agent needs to set the environmental variable SSH_AUTH_SOCK to /ssh/auth/sock and mount the ssh volume at path /ssh. Here is an example ssh.overrides.yml file which shows how to add SSH agent access to services from the build container . To use this file run docker-compose -f build.yml -f ssh.overrides.yml run --rm DESIRED_SERVICE Overrides and extends Overrides do not carry through to services extending an overridden service. The example below demonstrates that one needs to define the override for each service where it should apply even though in a typical build.yml file the cli service is a descendant of the base service. # ssh.overrides.yml version: '2.1' services: cli: volumes: - ssh:/ssh environment: SSH_AUTH_SOCK: /ssh/auth/sock base: volumes: - ssh:/ssh environment: SSH_AUTH_SOCK: /ssh/auth/sock volumes: ssh: external: true The SSH agent configuration can also be placed into your build.yml file directly if it is relevant for all team members. More Resources Additional information, including ways to have non-root container processes access the authorization socket can be found in the whilp/ssh-agent README image.","title":"SSH Keys for private repos"},{"location":"common-tasks/ssh-keys-for-private-repos/#ssh-keys-for-private-repositories","text":"","title":"SSH Keys for Private Repositories"},{"location":"common-tasks/ssh-keys-for-private-repos/#forwardimport-your-ssh-key-into-the-build-container-to-clone-private-repositories","text":"Certain containers like Jenkins and build may need your private key. This is supported by importing your private key into the container via a volume mount. To get your private key into the build container, volume mount your key into the container at /root/.ssh/outrigger.key and it will be processed accordingly. ~/.ssh/id_rsa:/root/.ssh/outrigger.key","title":"Forward/Import your SSH Key into the build container to clone private repositories"},{"location":"common-tasks/ssh-keys-for-private-repos/#passphrase-protected-keys","text":"For interactive operations, such as use of the build container, passphrase protected keys do not pose a challenge as you can enter the passphrase when prompted. Services like Jenkins, however, can prove problematic. A solution to this is to use a container running an SSH agent with your key added and unlocked. Your container will be able to connect with the SSH agent via the use of a shared volume and the SSH_AUTH_SOCK variable that points to it.","title":"Passphrase Protected Keys"},{"location":"common-tasks/ssh-keys-for-private-repos/#ssh-agent-container","text":"The following is an docker compose file, ssh.yml, with services defined to help run the necessary container for the agent and add your desired keys. It is set up for a single SSH agent to share with all of your projects. If you prefer project specific SSH agents, remove the external: true declaration in all sample files. In order to have a single agent for all projects, first create a named volume using docker volume create --name=ssh . This only needs to be done once and you'll be prompted to do so if you forget. # ssh.yml # Provides a service for the ssh-agent and a service for adding keys to it # # Agent will need to be restarted and have key(s) re-added to it any time your # docker host is restarted. # version: '3.2' services: # use via docker-compose -f ssh.yml up [-d] ssh-agent ssh-agent: container_name: ssh-agent image: whilp/ssh-agent:latest volumes: - ssh:/ssh # use via docker-compose -f ssh.yml run --rm ssh-add $HOME/.ssh/name_of_desired_key ssh-add: image: whilp/ssh-agent:latest entrypoint: [ ssh-add ] volumes: - ssh:/ssh - $HOME:$HOME volumes: ssh: external: true","title":"SSH Agent Container"},{"location":"common-tasks/ssh-keys-for-private-repos/#connecting-to-the-ssh-agent-from-other-containers","text":"Any container you want to connect to the SSH agent needs to set the environmental variable SSH_AUTH_SOCK to /ssh/auth/sock and mount the ssh volume at path /ssh. Here is an example ssh.overrides.yml file which shows how to add SSH agent access to services from the build container . To use this file run docker-compose -f build.yml -f ssh.overrides.yml run --rm DESIRED_SERVICE Overrides and extends Overrides do not carry through to services extending an overridden service. The example below demonstrates that one needs to define the override for each service where it should apply even though in a typical build.yml file the cli service is a descendant of the base service. # ssh.overrides.yml version: '2.1' services: cli: volumes: - ssh:/ssh environment: SSH_AUTH_SOCK: /ssh/auth/sock base: volumes: - ssh:/ssh environment: SSH_AUTH_SOCK: /ssh/auth/sock volumes: ssh: external: true The SSH agent configuration can also be placed into your build.yml file directly if it is relevant for all team members.","title":"Connecting to the SSH Agent from other containers"},{"location":"common-tasks/ssh-keys-for-private-repos/#more-resources","text":"Additional information, including ways to have non-root container processes access the authorization socket can be found in the whilp/ssh-agent README image.","title":"More Resources"},{"location":"common-tasks/starting-your-project-containers/","text":"Starting your project containers Configure your environment Ensure all terminals you intend to use can communicate with the Docker Host. Generally a simple docker ps will either list out running containers or give you an error like Cannot connect to the Docker daemon. Is the docker daemon running on this host? If you get the error make sure you have run eval \"$(rig config)\" . See Installation for the proper way to configure your environments. Start your containers In the project directory, start the containers with: docker-compose up Note The docker-compose command runs in the foreground as long as the Docker containers are running. (It is not hung if there is no output after \"Attaching [container name]...) Logs from the running containers will stream to the console and be prefixed by their compose names plus an integer (i.e. web_1, db_1, etc) You can start another terminal tab if you need to run other commands (such as the build container operations, etc.)","title":"Starting your project containers"},{"location":"common-tasks/starting-your-project-containers/#starting-your-project-containers","text":"","title":"Starting your project containers"},{"location":"common-tasks/starting-your-project-containers/#configure-your-environment","text":"Ensure all terminals you intend to use can communicate with the Docker Host. Generally a simple docker ps will either list out running containers or give you an error like Cannot connect to the Docker daemon. Is the docker daemon running on this host? If you get the error make sure you have run eval \"$(rig config)\" . See Installation for the proper way to configure your environments.","title":"Configure your environment"},{"location":"common-tasks/starting-your-project-containers/#start-your-containers","text":"In the project directory, start the containers with: docker-compose up Note The docker-compose command runs in the foreground as long as the Docker containers are running. (It is not hung if there is no output after \"Attaching [container name]...) Logs from the running containers will stream to the console and be prefixed by their compose names plus an integer (i.e. web_1, db_1, etc) You can start another terminal tab if you need to run other commands (such as the build container operations, etc.)","title":"Start your containers"},{"location":"common-tasks/stopping-containers-and-cleanup/","text":"Stopping containers and cleanup Stopping the containers for your project You may want to recoup the resources used by your projects containers and the docker host for other things if you are done with development for a while. You can stop the containers for a single project only or all containers and the docker host depending on what you are finished using. If you only want to stop the containers for your project, in the project directory, run docker-compose stop or press Ctrl-C to stop a docker-compose process running in the foreground and then run docker-compose stop to ensure the project containers have stopped. If you want to shut down the docker host as well as any containers, run rig stop Cleaning Up From time to time you'll want to clean up stopped containers. You'll also want to take special care when finishing a project to release all the resources used by it. For periodic cleanup of all stopped containers, run the following script while your docker host is running: rig prune If you only want to clean up project specific stopped containers, you can run: docker-compose rm from your project directory. When you are finished with a project, if you used any persistent data storage you'll want to run a command to clean it up. The exact directory to request removal from will depend on your project (see suggested directory naming guidelines in Key Concepts ): docker-machine ssh dev sudo rm -rf /data/[project]/","title":"Stopping containers and cleanup"},{"location":"common-tasks/stopping-containers-and-cleanup/#stopping-containers-and-cleanup","text":"","title":"Stopping containers and cleanup"},{"location":"common-tasks/stopping-containers-and-cleanup/#stopping-the-containers-for-your-project","text":"You may want to recoup the resources used by your projects containers and the docker host for other things if you are done with development for a while. You can stop the containers for a single project only or all containers and the docker host depending on what you are finished using. If you only want to stop the containers for your project, in the project directory, run docker-compose stop or press Ctrl-C to stop a docker-compose process running in the foreground and then run docker-compose stop to ensure the project containers have stopped. If you want to shut down the docker host as well as any containers, run rig stop","title":"Stopping the containers for your project"},{"location":"common-tasks/stopping-containers-and-cleanup/#cleaning-up","text":"From time to time you'll want to clean up stopped containers. You'll also want to take special care when finishing a project to release all the resources used by it. For periodic cleanup of all stopped containers, run the following script while your docker host is running: rig prune If you only want to clean up project specific stopped containers, you can run: docker-compose rm from your project directory. When you are finished with a project, if you used any persistent data storage you'll want to run a command to clean it up. The exact directory to request removal from will depend on your project (see suggested directory naming guidelines in Key Concepts ): docker-machine ssh dev sudo rm -rf /data/[project]/","title":"Cleaning Up"},{"location":"common-tasks/tbd/","text":"To Be Done","title":"To Be Done"},{"location":"common-tasks/tbd/#to-be-done","text":"","title":"To Be Done"},{"location":"common-tasks/upgrading-rig/","text":"Upgrading Outrigger Upgrade Outrigger CLI, rig Homebrew If you installed rig via Homebrew, you can check for updates by running brew update brew upgrade outrigger-cli Testing your installation Run rig doctor to confirm the system is in good working order. Upgrading your Docker Host If your brew upgrade happens to also update docker, when running rig doctor you may get a warning about an incompatible Docker version in your Docker Host VM. In that case use the rig upgrade command to upgrade your Docker Host VM to a compatible version.","title":"Upgrading Rig"},{"location":"common-tasks/upgrading-rig/#upgrading-outrigger","text":"","title":"Upgrading Outrigger"},{"location":"common-tasks/upgrading-rig/#upgrade-outrigger-cli-rig","text":"","title":"Upgrade Outrigger CLI, rig"},{"location":"common-tasks/upgrading-rig/#homebrew","text":"If you installed rig via Homebrew, you can check for updates by running brew update brew upgrade outrigger-cli","title":"Homebrew"},{"location":"common-tasks/upgrading-rig/#testing-your-installation","text":"Run rig doctor to confirm the system is in good working order.","title":"Testing your installation"},{"location":"common-tasks/upgrading-rig/#upgrading-your-docker-host","text":"If your brew upgrade happens to also update docker, when running rig doctor you may get a warning about an incompatible Docker version in your Docker Host VM. In that case use the rig upgrade command to upgrade your Docker Host VM to a compatible version.","title":"Upgrading your Docker Host"},{"location":"common-tasks/using-the-build-container/","text":"Using the Build Container Part of Outrigger is a build image. The idea of the build container is that it will have installed many of the tools you'd need to work on a project and the proper versions so that they work well together. Everything from drush to gem to npm to composer as well as grunt, bower, yeoman, etc. Providing these tools in a container means that you'll never have to worry about having the right tools installed on your laptop or integration environment to get your job done. The example Drupal project has a build.yml file that shows a variety of ways to use the container. Getting a build container command line One of the most common ways to use the build container is to launch a shell into the container and run commands from the command line interface (cli). The following command get you to the cli docker-compose -f build.yml run cli Making a command alias Using a shell alias you can make a really quick CLI command that is reusable across projects if you name your build.yml file the same everywhere. alias cli='docker-compose -f build.yml run cli then you only need to cli to get into your build containers shell. This trick can be used with all of the command below as well. Running a command container A \"command container\" is a term we have coined for a specific approach we use, where containers are spun up to run a single command in a consistent environment and then shut down. Here are some example of command containers that we have configured into the build.yml file from our example repository above. Drush Command Container This is a general purpose Drush command container that allows you to run any Drush command in a consistent environment docker-compose -f build.yml run drush [command here] Grunt Command Container This is a general purpose Grunt command container that allows you to run any grunt command in a consistent environment docker-compose -f build.yml run grunt [command here] Composer Command Container This is a general purpose Composer command container that allows you to run any composer command in a consistent environment docker-compose -f build.yml run composer [command here] Composer Install Command Container This one goes a step further when there is a specific command you want to run and control all the arguments used to run it. docker-compose -f build.yml run composer-install Share output from the build container Using the example project from above, we have a volume mount of your project code into the build container. Now, when you perform a build and it writes the output to the local project directory (e.g in ./build/html or something else in the working directory), those changes to the filesystem are effectively outside of the container. The web container should also define a volume mount for the build output into the docroot, or mount your project directory in the container and configure the docroot to be the absolute path to your build output and you will have seamless integration of build output to a web container running your project.","title":"Using the build container"},{"location":"common-tasks/using-the-build-container/#using-the-build-container","text":"Part of Outrigger is a build image. The idea of the build container is that it will have installed many of the tools you'd need to work on a project and the proper versions so that they work well together. Everything from drush to gem to npm to composer as well as grunt, bower, yeoman, etc. Providing these tools in a container means that you'll never have to worry about having the right tools installed on your laptop or integration environment to get your job done. The example Drupal project has a build.yml file that shows a variety of ways to use the container.","title":"Using the Build Container"},{"location":"common-tasks/using-the-build-container/#getting-a-build-container-command-line","text":"One of the most common ways to use the build container is to launch a shell into the container and run commands from the command line interface (cli). The following command get you to the cli docker-compose -f build.yml run cli Making a command alias Using a shell alias you can make a really quick CLI command that is reusable across projects if you name your build.yml file the same everywhere. alias cli='docker-compose -f build.yml run cli then you only need to cli to get into your build containers shell. This trick can be used with all of the command below as well.","title":"Getting a build container command line"},{"location":"common-tasks/using-the-build-container/#running-a-command-container","text":"A \"command container\" is a term we have coined for a specific approach we use, where containers are spun up to run a single command in a consistent environment and then shut down. Here are some example of command containers that we have configured into the build.yml file from our example repository above.","title":"Running a command container"},{"location":"common-tasks/using-the-build-container/#drush-command-container","text":"This is a general purpose Drush command container that allows you to run any Drush command in a consistent environment docker-compose -f build.yml run drush [command here]","title":"Drush Command Container"},{"location":"common-tasks/using-the-build-container/#grunt-command-container","text":"This is a general purpose Grunt command container that allows you to run any grunt command in a consistent environment docker-compose -f build.yml run grunt [command here]","title":"Grunt Command Container"},{"location":"common-tasks/using-the-build-container/#composer-command-container","text":"This is a general purpose Composer command container that allows you to run any composer command in a consistent environment docker-compose -f build.yml run composer [command here]","title":"Composer Command Container"},{"location":"common-tasks/using-the-build-container/#composer-install-command-container","text":"This one goes a step further when there is a specific command you want to run and control all the arguments used to run it. docker-compose -f build.yml run composer-install","title":"Composer Install Command Container"},{"location":"common-tasks/using-the-build-container/#share-output-from-the-build-container","text":"Using the example project from above, we have a volume mount of your project code into the build container. Now, when you perform a build and it writes the output to the local project directory (e.g in ./build/html or something else in the working directory), those changes to the filesystem are effectively outside of the container. The web container should also define a volume mount for the build output into the docroot, or mount your project directory in the container and configure the docroot to be the absolute path to your build output and you will have seamless integration of build output to a web container running your project.","title":"Share output from the build container"},{"location":"common-tasks/using-watches/","text":"Using Watches Often we need watches running inside of our containers. This could be for webpack, grunt, nodemon, etc. One of the main challenges with the NFS mounts used in Outrigger is that they do not forward filesystem notifications across the NFS mount and into containers, so we need better technology to facilitate that. rig now supports unison based file syncing between the local filesystem and a volume shared with a container. This setup allows multi-directional file syncing as well as full support for all filesystem notifications. See Filesystem Sync for details on how to setup Unison volumes.","title":"Using watches"},{"location":"common-tasks/using-watches/#using-watches","text":"Often we need watches running inside of our containers. This could be for webpack, grunt, nodemon, etc. One of the main challenges with the NFS mounts used in Outrigger is that they do not forward filesystem notifications across the NFS mount and into containers, so we need better technology to facilitate that. rig now supports unison based file syncing between the local filesystem and a volume shared with a container. This setup allows multi-directional file syncing as well as full support for all filesystem notifications. See Filesystem Sync for details on how to setup Unison volumes.","title":"Using Watches"},{"location":"common-tasks/using-xdebug-with-outrigger/","text":"Using Xdebug with Outrigger Getting Xdebug set up can be a bit challenging but while there are many discrete steps, they are individually straightforward. This guide will walk you through getting setup quickly. Examples below are with PhpStorm. Other IDEs should have similar steps and required configuration. Applies to use of the Outrigger Docker Images This documentation specifically pertains to using the Apache-PHP Image or the Build Image . rig itself is only relevant in that it brokers standardized DNS practices. Make sure your environment is up-to-date Consider updating rig before proceeding. Once done, run rig doctor to confirm Outrigger is healthy. Check out Troubleshooting or the F.A.Q. if anything comes up. If you haven't updated your Docker Images recently, doing so now is a good precautionary step to ensure you have versions that work with this documentation. Check out Routine Image Maintenance if you need a guide to updating. Conceptual Requirements This section details the end configuration needs for Xdebug and connection between the containers and your IDE. For both browser and CLI debugging your IDE needs to be configured to listen for the Xdebug connection and map paths of files as they appear in the container to how they appear to your IDE. Xdebug needs to be enabled and configured so that it knows how to talk to your IDE. If you are familiar with normal Xdebug setup, there are common steps which may be omitted. Specifically, there is no need to trigger the start of the debugger through query parameters, POST parameters, cookies or environmental variables. Containers are set to autostart debugging so as long as your IDE is listening debugging should begin automatically. Debugging Overhead The overhead from the attempted connection by Xdebug to your IDE is typically nominal. If you want to avoid it you can disable Xdebug by setting the PHP_XDEBUG environmental variable to \"false\". To allow debugging only when requested, override the PHP setting for xdebug.remote_autostart to disable the automatic connection. Browser-based Debugging For browser-based debugging ensure that your browser is connecting directly to the Apache-PHP container. This is because the IP address of the connection is used to connect back to your IDE. Chaining requests through intermediate containers like Varnish will result in the incorrect IP address getting used to attempt the connection to your IDE. Setup Steps The following section demonstrates required IDE configuration using PhpStorm. Other IDEs should have similar configuration options. The beginning of each section details the desired result of the configuration options. PhpStorm Configuration Adjust the Project's Debug Settings The first step is to configure your IDE to listen on the right port for connections from Xdebug, accept those connection attempts and control interactions with the debugger. Open your project's settings and go to Languages Frameworks - PHP - Debug . You can get to the project settings by going to: PhpStorm Preferences (OS X) or File Settings (Windows, Linux). Required Settings Debug port should be set to 9000. Can accept external connections should be enabled. Max. simultaneous connections: should be set greater than 1 to facilitate handling multiple requests at once. Command line debugging when using Drush usually requires multiple connections. Recommended Settings Ignore external connections through unregistered server configurations should be unselected to ease setup. When unselected, PhpStorm will prompt about connections with unrecognized configurations which can help troubleshoot configuration mistakes. Break at first line in PHP scripts can be set as desired. Without this checked, you will need to set breakpoints in your code. Force break at first line ... options should be selected to ease identification of missing configuration requirements. Set Up A Server Configuration for the Project The next step is to configure your IDE to translate between the paths for files it can see to the paths as they appear in the container to Xdebug. In your project's settings, go to Languages Frameworks - PHP - Servers . Create a new Server by clicking on the \"+\" button. Give your server a name and input the hostname of your container. It is recommended that you use the hostname as the server name. It will ease CLI debugging later. Be sure to add the docroot mappings. The example shown here is using the Grunt Drupal Tasks project structure. There are two mappings in this case. One for the docroot ( build/html ) and the other for the src directory so that breakpoints can be set in the custom modules in the src directory as well. These mappings are used to match paths from inside the Docker container to the paths used in the local filesystem where PhpStorm is run. Ensure That You IDE is Listening for Connections Set PhpStorm to listen for connections using the Run menu or hitting the listen button in the toolbar. It's the one that looks like a telephone. DBGp Proxy DBGp Proxy settings are not used when debugging Outrigger containers. Container Configuration Set the environmental variable PHP_XDEBUG to \"true\" for your container. If using a Docker Compose configuration file the setting will look something like the following: www: image: outrigger/apache-php:php71 ... environment: PHP_XDEBUG: true ... For debugging via the command line, also set the PHP_IDE_CONFIG environmental variable to a value of \"serverName=www.your-site.vm\" where the actual value used is the same as the name you gave your server configuration in your PhpStorm setup earlier. If using a Docker compose configuration file the setting will look something like the following: cli: image: outrigger/build:php71 ... environment: PHP_XDEBUG: true PHP_IDE_CONFIG: serverName=www.your-site.vm ... You'll need to restart your containers after making these changes. Debugging Your Configuration If things aren't working you can use the following tips for debugging. Examining Xdebug settings You can view your Xdebug configuration by looking inside the Apache container. With the container name (found via docker ps ), try running: docker exec [container_name] /usr/bin/env cat /etc/opt/remi/php70/php.d/15-xdebug.ini if using docker-compose with your Apache container named www , you can more simply run: docker-compose exec www /usr/bin/env cat /etc/opt/remi/php70/php.d/15-xdebug.ini This path varies by PHP version. For PHP 5.6 check /etc/opt/rh/rh-php56/php.d/15-xdebug.ini . The important pieces to look for are: For both browser-based and CLI debugging ; The extension is loaded zend_extension=xdebug.so ; Remote debugging is enabled xdebug.remote_enable=1 ; Debugging begins automatically xdebug.remote_autostart=1 ; Port should match your PhpStorm settings xdebug.remote_port=9000 ; Communication protocol should be dbgp xdebug.remote_handler=dbgp For CLI debugging, the following should also be present. The value of remote_host when debugging using a browser is dynamically set based on requesting IP so the configured value does not matter. xdebug.remote_host=192.168.99.1 Validate settings with PhpStorm Select the \"Web Server Debug Validation\" option from the \"Run\" menu option. (Confirm your Apache container is running or this validation will fail.) This will display a dialog window that allows you to validate your settings. Make sure that your \"Path to create validation script\" points to your project docroot and the URL is your project URL. If all goes well, clicking the \"Validate\" button should give you something like this: Click the dialog 'x' (close) button to close this dialog window. Restart PhpStorm or Containers Occasionally you may find that you need to restart PhpStorm or a container before connections succeed. Overriding the Default Xdebug Configuration If your project or workflow has special needs, you can override the Xdebug configuration using Volume Mounts to substitute your own template file. Copy the original template into your project and make the necessary changes. (You can also pull your current version of this file from the locally running docker image.) Commit your version of the file and add a volume mount to your docker-compose manifest with an entry such as: ./env/local/xdebug.ini.tmpl:/etc/confd/templates/xdebug.ini.tmpl Once that's in place, you will have to restart the container to pick up the new volume mount: docker-compose restart www","title":"Using Xdebug with Outrigger"},{"location":"common-tasks/using-xdebug-with-outrigger/#using-xdebug-with-outrigger","text":"Getting Xdebug set up can be a bit challenging but while there are many discrete steps, they are individually straightforward. This guide will walk you through getting setup quickly. Examples below are with PhpStorm. Other IDEs should have similar steps and required configuration. Applies to use of the Outrigger Docker Images This documentation specifically pertains to using the Apache-PHP Image or the Build Image . rig itself is only relevant in that it brokers standardized DNS practices. Make sure your environment is up-to-date Consider updating rig before proceeding. Once done, run rig doctor to confirm Outrigger is healthy. Check out Troubleshooting or the F.A.Q. if anything comes up. If you haven't updated your Docker Images recently, doing so now is a good precautionary step to ensure you have versions that work with this documentation. Check out Routine Image Maintenance if you need a guide to updating.","title":"Using Xdebug with Outrigger"},{"location":"common-tasks/using-xdebug-with-outrigger/#conceptual-requirements","text":"This section details the end configuration needs for Xdebug and connection between the containers and your IDE. For both browser and CLI debugging your IDE needs to be configured to listen for the Xdebug connection and map paths of files as they appear in the container to how they appear to your IDE. Xdebug needs to be enabled and configured so that it knows how to talk to your IDE. If you are familiar with normal Xdebug setup, there are common steps which may be omitted. Specifically, there is no need to trigger the start of the debugger through query parameters, POST parameters, cookies or environmental variables. Containers are set to autostart debugging so as long as your IDE is listening debugging should begin automatically. Debugging Overhead The overhead from the attempted connection by Xdebug to your IDE is typically nominal. If you want to avoid it you can disable Xdebug by setting the PHP_XDEBUG environmental variable to \"false\". To allow debugging only when requested, override the PHP setting for xdebug.remote_autostart to disable the automatic connection.","title":"Conceptual Requirements"},{"location":"common-tasks/using-xdebug-with-outrigger/#browser-based-debugging","text":"For browser-based debugging ensure that your browser is connecting directly to the Apache-PHP container. This is because the IP address of the connection is used to connect back to your IDE. Chaining requests through intermediate containers like Varnish will result in the incorrect IP address getting used to attempt the connection to your IDE.","title":"Browser-based Debugging"},{"location":"common-tasks/using-xdebug-with-outrigger/#setup-steps","text":"The following section demonstrates required IDE configuration using PhpStorm. Other IDEs should have similar configuration options. The beginning of each section details the desired result of the configuration options.","title":"Setup Steps"},{"location":"common-tasks/using-xdebug-with-outrigger/#phpstorm-configuration","text":"","title":"PhpStorm Configuration"},{"location":"common-tasks/using-xdebug-with-outrigger/#adjust-the-projects-debug-settings","text":"The first step is to configure your IDE to listen on the right port for connections from Xdebug, accept those connection attempts and control interactions with the debugger. Open your project's settings and go to Languages Frameworks - PHP - Debug . You can get to the project settings by going to: PhpStorm Preferences (OS X) or File Settings (Windows, Linux).","title":"Adjust the Project's Debug Settings"},{"location":"common-tasks/using-xdebug-with-outrigger/#required-settings","text":"Debug port should be set to 9000. Can accept external connections should be enabled. Max. simultaneous connections: should be set greater than 1 to facilitate handling multiple requests at once. Command line debugging when using Drush usually requires multiple connections.","title":"Required Settings"},{"location":"common-tasks/using-xdebug-with-outrigger/#recommended-settings","text":"Ignore external connections through unregistered server configurations should be unselected to ease setup. When unselected, PhpStorm will prompt about connections with unrecognized configurations which can help troubleshoot configuration mistakes. Break at first line in PHP scripts can be set as desired. Without this checked, you will need to set breakpoints in your code. Force break at first line ... options should be selected to ease identification of missing configuration requirements.","title":"Recommended Settings"},{"location":"common-tasks/using-xdebug-with-outrigger/#set-up-a-server-configuration-for-the-project","text":"The next step is to configure your IDE to translate between the paths for files it can see to the paths as they appear in the container to Xdebug. In your project's settings, go to Languages Frameworks - PHP - Servers . Create a new Server by clicking on the \"+\" button. Give your server a name and input the hostname of your container. It is recommended that you use the hostname as the server name. It will ease CLI debugging later. Be sure to add the docroot mappings. The example shown here is using the Grunt Drupal Tasks project structure. There are two mappings in this case. One for the docroot ( build/html ) and the other for the src directory so that breakpoints can be set in the custom modules in the src directory as well. These mappings are used to match paths from inside the Docker container to the paths used in the local filesystem where PhpStorm is run.","title":"Set Up A Server Configuration for the Project"},{"location":"common-tasks/using-xdebug-with-outrigger/#ensure-that-you-ide-is-listening-for-connections","text":"Set PhpStorm to listen for connections using the Run menu or hitting the listen button in the toolbar. It's the one that looks like a telephone.","title":"Ensure That You IDE is Listening for Connections"},{"location":"common-tasks/using-xdebug-with-outrigger/#dbgp-proxy","text":"DBGp Proxy settings are not used when debugging Outrigger containers.","title":"DBGp Proxy"},{"location":"common-tasks/using-xdebug-with-outrigger/#container-configuration","text":"Set the environmental variable PHP_XDEBUG to \"true\" for your container. If using a Docker Compose configuration file the setting will look something like the following: www: image: outrigger/apache-php:php71 ... environment: PHP_XDEBUG: true ... For debugging via the command line, also set the PHP_IDE_CONFIG environmental variable to a value of \"serverName=www.your-site.vm\" where the actual value used is the same as the name you gave your server configuration in your PhpStorm setup earlier. If using a Docker compose configuration file the setting will look something like the following: cli: image: outrigger/build:php71 ... environment: PHP_XDEBUG: true PHP_IDE_CONFIG: serverName=www.your-site.vm ... You'll need to restart your containers after making these changes.","title":"Container Configuration"},{"location":"common-tasks/using-xdebug-with-outrigger/#debugging-your-configuration","text":"If things aren't working you can use the following tips for debugging.","title":"Debugging Your Configuration"},{"location":"common-tasks/using-xdebug-with-outrigger/#examining-xdebug-settings","text":"You can view your Xdebug configuration by looking inside the Apache container. With the container name (found via docker ps ), try running: docker exec [container_name] /usr/bin/env cat /etc/opt/remi/php70/php.d/15-xdebug.ini if using docker-compose with your Apache container named www , you can more simply run: docker-compose exec www /usr/bin/env cat /etc/opt/remi/php70/php.d/15-xdebug.ini This path varies by PHP version. For PHP 5.6 check /etc/opt/rh/rh-php56/php.d/15-xdebug.ini . The important pieces to look for are: For both browser-based and CLI debugging ; The extension is loaded zend_extension=xdebug.so ; Remote debugging is enabled xdebug.remote_enable=1 ; Debugging begins automatically xdebug.remote_autostart=1 ; Port should match your PhpStorm settings xdebug.remote_port=9000 ; Communication protocol should be dbgp xdebug.remote_handler=dbgp For CLI debugging, the following should also be present. The value of remote_host when debugging using a browser is dynamically set based on requesting IP so the configured value does not matter. xdebug.remote_host=192.168.99.1","title":"Examining Xdebug settings"},{"location":"common-tasks/using-xdebug-with-outrigger/#validate-settings-with-phpstorm","text":"Select the \"Web Server Debug Validation\" option from the \"Run\" menu option. (Confirm your Apache container is running or this validation will fail.) This will display a dialog window that allows you to validate your settings. Make sure that your \"Path to create validation script\" points to your project docroot and the URL is your project URL. If all goes well, clicking the \"Validate\" button should give you something like this: Click the dialog 'x' (close) button to close this dialog window.","title":"Validate settings with PhpStorm"},{"location":"common-tasks/using-xdebug-with-outrigger/#restart-phpstorm-or-containers","text":"Occasionally you may find that you need to restart PhpStorm or a container before connections succeed.","title":"Restart PhpStorm or Containers"},{"location":"common-tasks/using-xdebug-with-outrigger/#overriding-the-default-xdebug-configuration","text":"If your project or workflow has special needs, you can override the Xdebug configuration using Volume Mounts to substitute your own template file. Copy the original template into your project and make the necessary changes. (You can also pull your current version of this file from the locally running docker image.) Commit your version of the file and add a volume mount to your docker-compose manifest with an entry such as: ./env/local/xdebug.ini.tmpl:/etc/confd/templates/xdebug.ini.tmpl Once that's in place, you will have to restart the container to pick up the new volume mount: docker-compose restart www","title":"Overriding the Default Xdebug Configuration"},{"location":"common-tasks/working-offline/","text":"Working Offline A common desire is the ability to leverage the full development environment provided to work offline. When attempting this users are often thwarted trying to access containers which appear to be successfully running. This issue is due to a failure to resolve domain names to the appropriate IP address for the container. This affects those using OS X and appears to be the result of a failure of the name resolution system to operate when no network interface appears connected. Typical error messages in browsers warn of being disconnected from the internet. Specifically, Chrome will include the error code ERR_INTERNET_DISCONNECTED. In these instances, a test command like curl -v http://www.project.vm/ succeeds as does attempting to use the IP address of a container within a browser. The command rig dns-records will display a mapping between all known containers and IP addresses. The docker inspect CONTAINER_NAME command can be used to view more detailed information about a specific container including its IP address and should be used on systems not supporting the rig application. Offline DNS Workaround The work around for this issue is to use the rig dns-records command and copy the output to your /etc/hosts file. You'll need to update this file any time you start or stop project containers and you should clean entries from it when you reconnect to a network. On systems which do not support the rig application the docker inspect command can be used to build a mapping of IP addresses to domain names. Network Changes Can Require Restart Network changes such as connecting or disconnecting an interface or VPN can require a restart of your Outrigger environment via rig restart or a re-execution of the rig dns command to ensure routing of traffic to containers and DNS resolution is properly configured. Additional Information Those seeking additional information about the root cause of this issue and wishing to explore potential solutions can read further information at the following URLs. Note that none of the purported solutions other than that described above has proven successful in testing. Note that some of these links refer to DNS utilities used in older OS X versions. http://serverfault.com/questions/22419/set-dns-server-on-os-x-even-when-without-internet-connection http://apple.stackexchange.com/questions/202887/mac-doesnt-use-local-dns-for-local http://superuser.com/questions/418833/using-dnsmasq-on-os-x-when-not-connected-to-the-internet http://apple.stackexchange.com/questions/26616/dns-not-resolving-on-mac-os-x This same issue affects other projects as well. https://github.com/basecamp/pow/issues/471 https://github.com/basecamp/pow/issues/104 A theoretical solution to this issue would be to have a network interface that always appeared active triggering the attempt to resolve the domain name. The following post discusses creation of a virtual interface to satisfy this requirement. The mechanism mentioned by bmasterswizzle and Alex Gray did not prove effective in testing. The interface could be created successfully but never brought up to a state where OS X considered it active. http://stackoverflow.com/questions/87442/virtual-network-interface-in-mac-os-x Some additional information and demonstrations of utilities for diagnostics can be found at http://apple.stackexchange.com/questions/26616/dns-not-resolving-on-mac-os-x http://serverfault.com/questions/478534/how-is-dns-lookup-configured-for-osx-mountain-lion","title":"Working Offline"},{"location":"common-tasks/working-offline/#working-offline","text":"A common desire is the ability to leverage the full development environment provided to work offline. When attempting this users are often thwarted trying to access containers which appear to be successfully running. This issue is due to a failure to resolve domain names to the appropriate IP address for the container. This affects those using OS X and appears to be the result of a failure of the name resolution system to operate when no network interface appears connected. Typical error messages in browsers warn of being disconnected from the internet. Specifically, Chrome will include the error code ERR_INTERNET_DISCONNECTED. In these instances, a test command like curl -v http://www.project.vm/ succeeds as does attempting to use the IP address of a container within a browser. The command rig dns-records will display a mapping between all known containers and IP addresses. The docker inspect CONTAINER_NAME command can be used to view more detailed information about a specific container including its IP address and should be used on systems not supporting the rig application.","title":"Working Offline"},{"location":"common-tasks/working-offline/#offline-dns-workaround","text":"The work around for this issue is to use the rig dns-records command and copy the output to your /etc/hosts file. You'll need to update this file any time you start or stop project containers and you should clean entries from it when you reconnect to a network. On systems which do not support the rig application the docker inspect command can be used to build a mapping of IP addresses to domain names. Network Changes Can Require Restart Network changes such as connecting or disconnecting an interface or VPN can require a restart of your Outrigger environment via rig restart or a re-execution of the rig dns command to ensure routing of traffic to containers and DNS resolution is properly configured.","title":"Offline DNS Workaround"},{"location":"common-tasks/working-offline/#additional-information","text":"Those seeking additional information about the root cause of this issue and wishing to explore potential solutions can read further information at the following URLs. Note that none of the purported solutions other than that described above has proven successful in testing. Note that some of these links refer to DNS utilities used in older OS X versions. http://serverfault.com/questions/22419/set-dns-server-on-os-x-even-when-without-internet-connection http://apple.stackexchange.com/questions/202887/mac-doesnt-use-local-dns-for-local http://superuser.com/questions/418833/using-dnsmasq-on-os-x-when-not-connected-to-the-internet http://apple.stackexchange.com/questions/26616/dns-not-resolving-on-mac-os-x This same issue affects other projects as well. https://github.com/basecamp/pow/issues/471 https://github.com/basecamp/pow/issues/104 A theoretical solution to this issue would be to have a network interface that always appeared active triggering the attempt to resolve the domain name. The following post discusses creation of a virtual interface to satisfy this requirement. The mechanism mentioned by bmasterswizzle and Alex Gray did not prove effective in testing. The interface could be created successfully but never brought up to a state where OS X considered it active. http://stackoverflow.com/questions/87442/virtual-network-interface-in-mac-os-x Some additional information and demonstrations of utilities for diagnostics can be found at http://apple.stackexchange.com/questions/26616/dns-not-resolving-on-mac-os-x http://serverfault.com/questions/478534/how-is-dns-lookup-configured-for-osx-mountain-lion","title":"Additional Information"},{"location":"common-tasks/working-with-volumes/","text":"Working with Volumes Volumes are a way that you can map directories or individual files into a running container. This is useful to provide code to run for a generic container, or directories to store data that persist longer than the life of the container, or to even override directories and/or files that exist in an image. Lets look at a few of those examples. Provide code for a generic container to run* The default phase2/apache-php:php70 is a Web/PHP container that provides only an index file in /var/www/html that prints out phpinfo(). This is obviously not very useful for an application, so we can provide an entire Drupal site to run and we do that by mapping our site into the default docroot like this: ./build/html:/var/www/html This takes the Drupal site we have in our local project directory of build/html and it overrides the default content of the images /var/www/html directory. Persist data longer than the life of the container When using a database like MySQL the container will store the database files in /var/lib/mysql and by default that directory will be reset every time the container restarts. This means each time you restart your container you'd need to reinstall your application and database, which can make life difficult. So in order to persist MySQL data for longer than the current run of the container we will map a directory from the Docker Host into the container and override the default /var/lib/mysql directory. The configuration will look something like this. /data/drupal/mysql:/var/lib/mysql Now when your database container creates files in /var/lib/mysql they are actually saved in the /data/drupal/mysql directory on the Docker Host. Be sure to namespace your directories inside /data so that separate projects don't conflict with each other. You'll also want to ensure you clean up when you are done with your project to keep from using up all of your disk space. See the cleaning up section for more info. Override directories and files that exist in an image Generally the configuration shipped with a Docker Image is meant to be the production configuration. Often, that configuration is not suitable for development and we need to override configurations. With volumes we have shown earlier how you can override directories, but you can also override individual files too. ./config/dev/httpd/httpd.conf:/etc/httpd/httpd.conf This takes a local httpd.conf file from our project and overrides the /etc/httpd/httpd.conf files that ships with the container. Changing Volume Definitions in Compose File If you wind up changing volume definitions in your docker-compose file you will need to remove your container before it will recognize those changes on a restart. In the project directory, run: docker-compose rm This command will remove all stopped containers defined in the docker-compose.yml. You could also remove an individual container by running: docker-compose rm name in yml After removing the container, it will revert to the original state from the Docker image, removing any packages installed or files modified that are not included in a volume mount. Start your containers again with docker-compose up and you should have your new volume mounts.","title":"Working with Volumes"},{"location":"common-tasks/working-with-volumes/#working-with-volumes","text":"Volumes are a way that you can map directories or individual files into a running container. This is useful to provide code to run for a generic container, or directories to store data that persist longer than the life of the container, or to even override directories and/or files that exist in an image. Lets look at a few of those examples.","title":"Working with Volumes"},{"location":"common-tasks/working-with-volumes/#provide-code-for-a-generic-container-to-run","text":"The default phase2/apache-php:php70 is a Web/PHP container that provides only an index file in /var/www/html that prints out phpinfo(). This is obviously not very useful for an application, so we can provide an entire Drupal site to run and we do that by mapping our site into the default docroot like this: ./build/html:/var/www/html This takes the Drupal site we have in our local project directory of build/html and it overrides the default content of the images /var/www/html directory.","title":"Provide code for a generic container to run*"},{"location":"common-tasks/working-with-volumes/#persist-data-longer-than-the-life-of-the-container","text":"When using a database like MySQL the container will store the database files in /var/lib/mysql and by default that directory will be reset every time the container restarts. This means each time you restart your container you'd need to reinstall your application and database, which can make life difficult. So in order to persist MySQL data for longer than the current run of the container we will map a directory from the Docker Host into the container and override the default /var/lib/mysql directory. The configuration will look something like this. /data/drupal/mysql:/var/lib/mysql Now when your database container creates files in /var/lib/mysql they are actually saved in the /data/drupal/mysql directory on the Docker Host. Be sure to namespace your directories inside /data so that separate projects don't conflict with each other. You'll also want to ensure you clean up when you are done with your project to keep from using up all of your disk space. See the cleaning up section for more info.","title":"Persist data longer than the life of the container"},{"location":"common-tasks/working-with-volumes/#override-directories-and-files-that-exist-in-an-image","text":"Generally the configuration shipped with a Docker Image is meant to be the production configuration. Often, that configuration is not suitable for development and we need to override configurations. With volumes we have shown earlier how you can override directories, but you can also override individual files too. ./config/dev/httpd/httpd.conf:/etc/httpd/httpd.conf This takes a local httpd.conf file from our project and overrides the /etc/httpd/httpd.conf files that ships with the container.","title":"Override directories and files that exist in an image"},{"location":"common-tasks/working-with-volumes/#changing-volume-definitions-in-compose-file","text":"If you wind up changing volume definitions in your docker-compose file you will need to remove your container before it will recognize those changes on a restart. In the project directory, run: docker-compose rm This command will remove all stopped containers defined in the docker-compose.yml. You could also remove an individual container by running: docker-compose rm name in yml After removing the container, it will revert to the original state from the Docker image, removing any packages installed or files modified that are not included in a volume mount. Start your containers again with docker-compose up and you should have your new volume mounts.","title":"Changing Volume Definitions in Compose File"},{"location":"faq/docker-for-mac/","text":"Docker for Mac Support Docker for Mac is a native hypervisor implementation of Docker that does not rely on a virtual machine provided by Docker Machine. It is new with some limitations and potential conflicts with Outrigger. We will highlight the path to a peaceful coexistence. Using only the Docker for Mac Binaries Docker for Mac provides docker and docker-compose binaries in /usr/local/bin . This install location conflicts with the binaries provided by Homebrew, but Outrigger can use these binaries too. If you have installed Docker for Mac first, you will get errors when trying to install the docker and docker-compose binaries via Homebrew. Those errors are fine, as long as when all is said and done docker , docker-compose , and docker-machine are all available on your $PATH . To use the binaries with your Outrigger VM you simply need to run the eval \"$(rig config)\" command to setup your Docker environment. This will point the docker to your VM. You will need to do this in every shell that you want to use the Outrigger VM. If you then need to transition back to using the Docker for Mac hypervisor, you will need to unset the Outrigger docker configuration. Use eval \"$(docker-machine env -u)\" to unset all DOCKER_* environment variables. If you need to do this often, we recommend setting up aliases to set/unset the environment vars. alias re='eval $(rig config) ' alias ru='eval $(docker-machine env -u) ' Using the Docker for Mac Binaries and Hypervisor To use the Docker for Mac Hypervisor follow the basic instructions below. Docker for Mac is less performant (early 2017) The FUSE driver they use to share your host filesystem into the hypervisor/containers is not nearly as performant as the NFS mount we use with the Outrigger VM. Therefore, operations that do either a lot of filesystem reading (like directory scan) or writing (like restoring DB dumps) will take much longer to perform. It is advisable to use the Outrigger VM / VirtualBox for the time being simply for performance reasons. Networking limitation when using the Docker for Mac Hypervisor There are some known limitations with the way networking is implemented with Docker for Mac. Most notable we can not directly access containers on their native IP address due to the lack of the docker0 bridge network that exists in the Docker Machine VM implementation. These limitations inhibit the fluid environment that Outrigger enables and as such is not natively supported (yet). We have done our best to highlight the issues and some of our ideas for workarounds below. Let us know how it goes if you venture down this path. Setup /data Outrigger makes a convention out of a /data directory within the VM. To provide the same directory to Docker for Mac, create a /data directory in the root of your Mac filesystem. You may need to open up the permissions so the container(s) can write to it. sudo mkdir /data sudo chmod 777 /data Once you have created that directory, go into Docker for Mac Preferences File Sharing and add /data to the list of shares and Apply Restart Docker for Mac. With this /data directory setup your Compose files should work on either the Docker for Mac Hypervisor or the Outrigger VM. Accessing your Container Services Docker for Mac does not provide a mechanism to route network traffic directly to your containers, they only support publishing (binding) ports from your running containers to your host. This means that all services are accessed on localhost (127.0.0.1) and share the same port space. (Two web server can't both publish to port 80). The approach we think makes sense (but don't actively support) is: Run jwilder/nginx-proxy as your only service that binds to port 80 Start all of your web containers with the VIRTUAL_HOST environment variable used by nginx-proxy to specify it's domain name Resolve the domain name from the above VIRTUAL_HOST variable in one of two ways Edit your /etc/hosts file and point the domain name at 127.0.0.1 Use dnsmasq Make sure dnsdock is not running In /etc/resolver/vm have nameserver 127.0.0.1 Run a dnsmasq container, bind it to port 53, configured to resolve all .vm addresses to 127.0.0.1 Uninstalling Docker for Mac Since Docker for Mac and Homebrew both install the Docker binaries in to /usr/local/bin after you uninstall Docker for Mac your Outrigger environment wont work. You may see an error like command not found: docker . The Docker for Mac installation overwrote the Homebrew based Docker installation, but brew still believes they are installed, so you'll need to unlink and re-link. brew unlink docker brew link docker brew unlink docker-compose brew link docker-compose brew unlink docker-machine brew link docker-machine","title":"Docker for Mac"},{"location":"faq/docker-for-mac/#docker-for-mac-support","text":"Docker for Mac is a native hypervisor implementation of Docker that does not rely on a virtual machine provided by Docker Machine. It is new with some limitations and potential conflicts with Outrigger. We will highlight the path to a peaceful coexistence.","title":"Docker for Mac Support"},{"location":"faq/docker-for-mac/#using-only-the-docker-for-mac-binaries","text":"Docker for Mac provides docker and docker-compose binaries in /usr/local/bin . This install location conflicts with the binaries provided by Homebrew, but Outrigger can use these binaries too. If you have installed Docker for Mac first, you will get errors when trying to install the docker and docker-compose binaries via Homebrew. Those errors are fine, as long as when all is said and done docker , docker-compose , and docker-machine are all available on your $PATH . To use the binaries with your Outrigger VM you simply need to run the eval \"$(rig config)\" command to setup your Docker environment. This will point the docker to your VM. You will need to do this in every shell that you want to use the Outrigger VM. If you then need to transition back to using the Docker for Mac hypervisor, you will need to unset the Outrigger docker configuration. Use eval \"$(docker-machine env -u)\" to unset all DOCKER_* environment variables. If you need to do this often, we recommend setting up aliases to set/unset the environment vars. alias re='eval $(rig config) ' alias ru='eval $(docker-machine env -u) '","title":"Using only the Docker for Mac Binaries"},{"location":"faq/docker-for-mac/#using-the-docker-for-mac-binaries-and-hypervisor","text":"To use the Docker for Mac Hypervisor follow the basic instructions below. Docker for Mac is less performant (early 2017) The FUSE driver they use to share your host filesystem into the hypervisor/containers is not nearly as performant as the NFS mount we use with the Outrigger VM. Therefore, operations that do either a lot of filesystem reading (like directory scan) or writing (like restoring DB dumps) will take much longer to perform. It is advisable to use the Outrigger VM / VirtualBox for the time being simply for performance reasons. Networking limitation when using the Docker for Mac Hypervisor There are some known limitations with the way networking is implemented with Docker for Mac. Most notable we can not directly access containers on their native IP address due to the lack of the docker0 bridge network that exists in the Docker Machine VM implementation. These limitations inhibit the fluid environment that Outrigger enables and as such is not natively supported (yet). We have done our best to highlight the issues and some of our ideas for workarounds below. Let us know how it goes if you venture down this path.","title":"Using the Docker for Mac Binaries and Hypervisor"},{"location":"faq/docker-for-mac/#setup-data","text":"Outrigger makes a convention out of a /data directory within the VM. To provide the same directory to Docker for Mac, create a /data directory in the root of your Mac filesystem. You may need to open up the permissions so the container(s) can write to it. sudo mkdir /data sudo chmod 777 /data Once you have created that directory, go into Docker for Mac Preferences File Sharing and add /data to the list of shares and Apply Restart Docker for Mac. With this /data directory setup your Compose files should work on either the Docker for Mac Hypervisor or the Outrigger VM.","title":"Setup /data"},{"location":"faq/docker-for-mac/#accessing-your-container-services","text":"Docker for Mac does not provide a mechanism to route network traffic directly to your containers, they only support publishing (binding) ports from your running containers to your host. This means that all services are accessed on localhost (127.0.0.1) and share the same port space. (Two web server can't both publish to port 80). The approach we think makes sense (but don't actively support) is: Run jwilder/nginx-proxy as your only service that binds to port 80 Start all of your web containers with the VIRTUAL_HOST environment variable used by nginx-proxy to specify it's domain name Resolve the domain name from the above VIRTUAL_HOST variable in one of two ways Edit your /etc/hosts file and point the domain name at 127.0.0.1 Use dnsmasq Make sure dnsdock is not running In /etc/resolver/vm have nameserver 127.0.0.1 Run a dnsmasq container, bind it to port 53, configured to resolve all .vm addresses to 127.0.0.1","title":"Accessing your Container Services"},{"location":"faq/docker-for-mac/#uninstalling-docker-for-mac","text":"Since Docker for Mac and Homebrew both install the Docker binaries in to /usr/local/bin after you uninstall Docker for Mac your Outrigger environment wont work. You may see an error like command not found: docker . The Docker for Mac installation overwrote the Homebrew based Docker installation, but brew still believes they are installed, so you'll need to unlink and re-link. brew unlink docker brew link docker brew unlink docker-compose brew link docker-compose brew unlink docker-machine brew link docker-machine","title":"Uninstalling Docker for Mac"},{"location":"faq/general/","text":"General FAQ How do I configure my web container / virtual hosts so I can run multiple projects at once from the same web container You don\u2019t. Seriously. This is a place where you must change your thinking from that of treating a server as a unit to treating the service as a unit. If you have multiple projects active at once, you\u2019ll have a web serving container active for each of them. This is OK because containers are much lighter weight than full virtual machines, though you do want to be careful about starting too many at once. Consider that you will have a docker-compose file for each project you are working on, and each docker-compose file represents your application and all the services required to run it. You could be running multiple docker-compose applications on a single Docker Host. Realistically, for the performance of your computer you would stop one docker-compose environment before you would run another. How do I see what containers are running on my Docker Host If you want to see how many containers are present within your Docker Host VM and check on the status of them just run docker ps This will show you the containers running, the image they are based on, the ports they expose and the name of the container. To see all the containers on the Docker Host both running and stopped use the command docker ps -a Why do I have so many containers / cleanup Any time you finish a project or you need to reset things, you should clean up your containers for a project by running docker-compose rm . That will remove the instances for the containers specified in your docker compose file. Additionally you can run rig prune to clear out all stopped containers and any dangling images. Is there any regular maintenance needed Yes, to keep your environment working smoothly and with current infrastructure configurations, implement a personal regimen of Routine Image Maintenance . Working with multiple Docker versions [Homebrew] If you have multiple versions installed, doing brew info docker-compose will list the versions you have installed. The one with the \"*\" will be the version that is active. If you need to switch versions, use the brew switch command and run any additional commands like brew unlink ... . (The switch command will provide next command steps if you need to link or unlink any formulae). Example: brew switch docker 1.13.1 Can I get .vm container names a Docker Host not created with Outrigger Yes, with an important caveat that containers will not be able to resolve .vm addresses without reconfiguring the daemon. The rig dns command can accept a --name parameter to run the DNS services on an existing Docker Host When we start a machine with rig start , we do the following things: Start a machine with the -dns=172.17.0.1 Docker daemon option set so that all containers will try dnsdock for DNS Run dnsdock bound to 172.17.0.1:53 to provide .vm addresses for all containers Set up Mac OS X to route 172.17.0.1/16 to the Docker virtual machine's IP so the host machine can access containers direct Set up /etc/resolver/vm so that OS X will look up .vm addresses through DNS queries to 172.17.0.1 If you run rig --name=$DOCKER_MACHINE_NAME dns , every step except the first will run, so your OS X host will be able to reach containers by their .vm addresses but other containers will not. This is the major difference between creating a machine with rig start and applying the DNS configuration to an existing machine with rig dns . Monitoring of containers If you need some insight into how many resources a given Docker container may be using, take advantage of the command docker stats container name . This handy command will show you CPU%, Memory%, Memory Usage vs Limit, and Network I/O. It is rudimentary but can be very useful in the first line of inspection on a container.","title":"General"},{"location":"faq/general/#general-faq","text":"","title":"General FAQ"},{"location":"faq/general/#how-do-i-configure-my-web-container-virtual-hosts-so-i-can-run-multiple-projects-at-once-from-the-same-web-container","text":"You don\u2019t. Seriously. This is a place where you must change your thinking from that of treating a server as a unit to treating the service as a unit. If you have multiple projects active at once, you\u2019ll have a web serving container active for each of them. This is OK because containers are much lighter weight than full virtual machines, though you do want to be careful about starting too many at once. Consider that you will have a docker-compose file for each project you are working on, and each docker-compose file represents your application and all the services required to run it. You could be running multiple docker-compose applications on a single Docker Host. Realistically, for the performance of your computer you would stop one docker-compose environment before you would run another.","title":"How do I configure my web container / virtual hosts so I can run multiple projects at once from the same web container"},{"location":"faq/general/#how-do-i-see-what-containers-are-running-on-my-docker-host","text":"If you want to see how many containers are present within your Docker Host VM and check on the status of them just run docker ps This will show you the containers running, the image they are based on, the ports they expose and the name of the container. To see all the containers on the Docker Host both running and stopped use the command docker ps -a","title":"How do I see what containers are running on my Docker Host"},{"location":"faq/general/#why-do-i-have-so-many-containers-cleanup","text":"Any time you finish a project or you need to reset things, you should clean up your containers for a project by running docker-compose rm . That will remove the instances for the containers specified in your docker compose file. Additionally you can run rig prune to clear out all stopped containers and any dangling images.","title":"Why do I have so many containers / cleanup"},{"location":"faq/general/#is-there-any-regular-maintenance-needed","text":"Yes, to keep your environment working smoothly and with current infrastructure configurations, implement a personal regimen of Routine Image Maintenance .","title":"Is there any regular maintenance needed"},{"location":"faq/general/#working-with-multiple-docker-versions-homebrew","text":"If you have multiple versions installed, doing brew info docker-compose will list the versions you have installed. The one with the \"*\" will be the version that is active. If you need to switch versions, use the brew switch command and run any additional commands like brew unlink ... . (The switch command will provide next command steps if you need to link or unlink any formulae). Example: brew switch docker 1.13.1","title":"Working with multiple Docker versions [Homebrew]"},{"location":"faq/general/#can-i-get-vm-container-names-a-docker-host-not-created-with-outrigger","text":"Yes, with an important caveat that containers will not be able to resolve .vm addresses without reconfiguring the daemon. The rig dns command can accept a --name parameter to run the DNS services on an existing Docker Host When we start a machine with rig start , we do the following things: Start a machine with the -dns=172.17.0.1 Docker daemon option set so that all containers will try dnsdock for DNS Run dnsdock bound to 172.17.0.1:53 to provide .vm addresses for all containers Set up Mac OS X to route 172.17.0.1/16 to the Docker virtual machine's IP so the host machine can access containers direct Set up /etc/resolver/vm so that OS X will look up .vm addresses through DNS queries to 172.17.0.1 If you run rig --name=$DOCKER_MACHINE_NAME dns , every step except the first will run, so your OS X host will be able to reach containers by their .vm addresses but other containers will not. This is the major difference between creating a machine with rig start and applying the DNS configuration to an existing machine with rig dns .","title":"Can I get .vm container names a Docker Host not created with Outrigger"},{"location":"faq/general/#monitoring-of-containers","text":"If you need some insight into how many resources a given Docker container may be using, take advantage of the command docker stats container name . This handy command will show you CPU%, Memory%, Memory Usage vs Limit, and Network I/O. It is rudimentary but can be very useful in the first line of inspection on a container.","title":"Monitoring of containers"},{"location":"faq/how-to-contribute/","text":"How to Contribute The details are TBD, but you can start by collaborating with us at https://github.com/phase2/rig","title":"How to contribute"},{"location":"faq/how-to-contribute/#how-to-contribute","text":"The details are TBD, but you can start by collaborating with us at https://github.com/phase2/rig","title":"How to Contribute"},{"location":"faq/troubleshooting/","text":"Troubleshooting See the following sections for common problems and ways to solve them. Run doctor Run rig doctor to determine if your environment is set to run Outrigger. Ensure the environment is setup correctly It can be useful to ensure everything is in a clean state. The following should ensure that rig stop stops the docker machine and cleans up networking eval \"$(rig config)\" clears environmental variables docker uses to communicate with the docker host rig start starts the docker machine eval \"$(rig config)\" sets environmental variables docker uses to communicate with the docker host Ensure your images are up to date From time to time images are updated to fix bugs or add functionality. You won't automatically receive these updates but you can fetch them when you hear new ones are available. docker pull imagename can be used if you want to update a specific image. For example, if you wanted to make sure you had the latest mariadb you'd run docker pull phase2/mariadb . docker-compose pull can be used within a project directory to make sure you've got the latest version of all images in the docker-compose.yml file. Configure Your Shell If you do not have any containers listed when running docker ps or you get an error message like: Get http:///var/run/docker.sock/v1.20/containers/json: dial unix /var/run/docker.sock: no such file or directory. * Are you trying to connect to a TLS-enabled daemon without TLS? * Is your docker daemon up and running? Or an error message like: Couldn't connect to Docker daemon - you might need to run `boot2docker up`. Make sure your shell has the necessary environment variables by running: eval \"$(rig config)\" Reset everything If a problem continue to persists and the Docker Host is non-responsive, you may need to resort to the nuclear option of blowing everything away and starting over. Note this is a nuclear option as even your persistent data area will be removed if you don't back it up. If you have any data that needs to be maintained be sure to get a copy of it off of your VM first with the scripts provided. To wipe everything out and start over First backup your existing data (if desired) by running rig data-backup . This will sync your entire /data directory to your host machine. Then you can run rig remove This removes the broken Docker Host and it\u2019s state, making way for a clean start. Next you can rebuild everything by running rig start . This will create you new Docker Host. If you wish to restore the /data directory that you previously backed up, then run rig data-restore Files Not Found If you get messages about files not being found in the container that should be shared from your host computer, check the following: Is the project checked out under the /Users folder? Only files under the /Users folder are shared into the Docker Host (and thus containers) by default. Does the docker-compose.yml file have a volume mount set up that contains the missing files? Get shell in the Docker container via docker exec -it container name /bin/bash and check if the files are shared in the wrong place. Get shell in the Docker Machine with docker-machine ssh dev and check if you find the files under /Users. Image not found If you encounter the following error about the Docker image not being found when starting a project, it may indicate that the image is private and your Docker client is not logged into a required private Docker Hub repository: Pulling repository phase2/privateimage Error: image phase2/privateimage:latest not found To solve this, run docker login and provide the relevant credentials. Docker client and server version incompatibilities If you see an error similar to this: Error response from daemon: client is newer than server (client API version: 1.20, server API version: 1.19) You likely need to upgrade your Docker Host to a get Docker client API compatibility. Do that with: `rig upgrade` Network timed out and can't pull container image Example: Pulling cache (phase2/memcache:latest)... Pulling repository docker.io/phase2/memcache Network timed out while trying to connect to https://index.docker.io/v1/repositories/phase2/memcache/images. You may want to check your internet connection or if you are behind a proxy. Try restarting the Docker Machine: rig restart Started machines may have a new IP address Example: Started machines may have new IP addresses. You may need to re-run the `docker-machine env` command. [WARN] Docker daemon not running! Trying again in 3 seconds. Try 1 of 10. ... [WARN] Docker daemon not running! Trying again in 3 seconds. Try 10 of 10. [ERROR] Docker daemon failed to start! The Docker Host (probably called dev) has it's IP changed. The generated TLS Certs are no longer valid and must be regenerated. Possible causes or relations/patterns: Another VM was started in VirtualBox. Machine went to sleep and somehow caused issues with the running VM. Fix: Try running: docker-machine env If that does not work, you should be able to start the VM directly through docker-machine: docker-machine start dev Next, check the IP / TLS status by running: docker-machine ls The output will likely report something akin to: dev - virtualbox Running tcp://192.168.99.100:2376 Unknown Unable to query docker version: Get https://192.168.99.100:2376/v1.15/version: x509: certificate is valid for 192.168.99.101, not 192.168.99.100 Now, regenerate the TLS Certs: docker-machine regenerate-certs dev -f Devtools should now be able to start. Don't forget to run eval \"$(rig config)\" after. Containers Started but Service Not Available Your Docker Host is running, the project's containers are up, and command-line operations work fine. Why can't you view the site in your web browser? DNS Services The DNS services that Outrigger spins up may not be working. Run docker ps and see that you have a dnsdock and dnsmasq container. If those services are not running, try rig dns to bring them up, or a full rig restart if that does not work. rig dns-records is also useful to see what containers have registered names. DNS Configuration It is also possible that the DNS services are running for your environment, but somehow the configuration is wrong. Run rig dns-records and make sure your project has an entry. If not, you may need another restart, or perhaps you are missing com.dnsdock.name and com.dnsdock.image labels in your docker-compose.yml? Slow-starting Services Some services, such as Apache Solr or Varnish, can take longer to start up than Apache and PHP-FPM. As a result you might load the browser so fast that not all services are available, which in the case of a proxy may prevent the page from loading at all. Wait a short time and try reloading the page. Failed Health Checks Some services such as Varnish depend on others to operate, and have built-in health checks to verify the other service is operating. If such a health check fails, there could be two problems: The internal DNS routing between Docker containers is broken. Make sure the configuration of your services is correct. The dependency (e.g., Apache behind Varnish) is not yet up and running when Varnish performs its checks. In either case, you can often repair the problem by performing a clean restart of the broken service. docker-compose stop proxy docker-compose rm -f proxy docker-compose up -d proxy Your other services should already be up and functional when this is done, so the health check will not fail on account of (2). Checking on Varnish Health If you suspect Varnish may be failing, run docker exec -it [VARNISH_CONTAINER] varnishlog and scan for VCL compilation errors. Service Became Non-Responsive Sometimes a service locks up. Apache stops serving results, Solr stops indexing or responding to search queries. These things happen on servers all the time. It may even happen more often on Docker, especially since we are now using more \"infrastructure\" in our local environments. Docker is meant to easily sandbox these problems from the rest of your machine, and to easily resolve these problems by allowing you to dump the problem and start over fresh very easily. Hard reset on a service (as describe above), is very much like a power cycle: docker-compose stop [BROKEN_SERVICE] docker-compose rm -f [BROKEN_SERVICE] docker-compose up -d [BROKEN_SERVICE] Sometimes data for a service is volume mounted from outside the container. This persistence is good when the data is healthy, but can be really bad if the data is part of the problem (e.g., broken lockfiles). docker-machine ssh [MACHINE_NAME] rm -Rf /data/[PROJECT]/[DIRECTORY_FOR_SERVICE] Remember that your service may have configuration or data in another service, you may need to perform this operation against multiple containers (e.g., Solr) NFS Conflicts with other Environments Running the Devtools VM in conjunction with other development environments (such as Vagrant) can sometimes cause conflicts with volume mounts. This is due to the fact that the Devtools VM mounts the entirety of /Users into the VM on OS X. Additional NFS mounts from other environments that target subdirectories of /Users will fail. The easiest workaround is to keep projects that use non-Devtools environments like Vagrant in a directory outside of /Users , such as /opt . Alternatively, you can run a single VM at a time and manually clear out /etc/exports prior to switching environments/projects. Migrating Vagrant boxes outside of /Users If you'd like to move your vagrant boxes outside of your home directory, perform the following steps: Choose a destination folder. We'll be using /opt/vms for this example. Insure the destination has enough free space. Typically this is not a problem because our Macs are one single partition. Make the new directory, and ensure your userid owns it: sudo mkdir /opt/vms; sudo chown -R userid:userid /opt/vms Add export VAGRANT_HOME=/opt/vms to ~/.bash_profile Move your vagrant folders over to /opt/vms. edit /etc/exports, updating any vagrant mount point to use the new location. (example: /Users/userid/vagrant/projectx/cms) While a reboot isn't necessary, it'll help to make sure nothing is running and using the old mount points. cd into your new vagrant directory, and do a vagrant up. Permission Errors During Container Startup NFS mounts from the host volume into a container may require a specific UID mapping in order for proper permission setup within the container. SSH key volume mounts are common in project build containers. If there's a problem with permissions for host files or directories being mounted inside the docker-machine via NFS, it could be a UID mapping problem. One way this problem manifests itself is, given a mount entry in a .yml file like: volumes: - ~/.ssh/id_rsa:/root/.ssh/id_rsa When starting the container, you may see an error like: ERROR: Cannot start service cli: error while creating mount source path '/Users/username/.ssh/id_rsa': mkdir /Users/username/.ssh/id_rsa: permission denied Verify that the UID of the OSX directory is mapped correctly in /etc/exports . In the host operating system (example is for OSX): $ id uid=502(username) gid=20(staff) groups=20(staff),... In /etc/exports: # docker-machine-nfs-begin machine-name /Users 192.168.99.100 -alldirs -mapall=501:20 # -- incorrect # docker-machine-nfs-end machine-name The -mapall arguments need to match your uid and gid from the id command. If you need to change /etc/exports (in the host), edit it, save the changes, then restart the nfsd : rig stop sudo nfsd restart rig start Failures during project sync startup The initial sync process involves scanning your codebase to assemble a list of all the files it contains. For larger codebases or slower computers this process may not complete before a timeout is reached. The timeout attempts to ensure that if there is a problem that prevents the unison container or local process from starting you don't wait without realizing there is an issue. To see if you are getting an erroneous message message you can look to see if a local unison process and a unison container for your project eventually start. The easiest check can be to run the rig project sync command with the initial-sync-timeout option set to a large value such as 300 seconds. If you want to dig deeper, look for a local unison process on your host machine with your project sync volume name as a log file. For example: ps aux | grep myproject-sync.log . There should also be a docker container running with a name like myproject-sync. If both of those are present it is possible that a timeout that is too short is affecting you. Identifying and troubleshooting sync issues When the unison filesystem based syncing stops working it unfortunately does so silently. If the files your container is using don't seem to be updating when you think they should it may be that the syncing has stopped working. This has typically been observed happening when putting the host machine into sleep mode. It may also happen if something causes either side of the syncing process to crash due to a very high rate of filesystem changes. Simply restarting the sync usually resolves the issue. As of version 2.2.0, Outrigger has commands to help determine the state of the filesystem syncing a as well as commands that facilitate scripting and cleanup. See the Additional Sync Commands section of Filesystem Sync for information about these commands.","title":"Troubleshooting"},{"location":"faq/troubleshooting/#troubleshooting","text":"See the following sections for common problems and ways to solve them.","title":"Troubleshooting"},{"location":"faq/troubleshooting/#run-doctor","text":"Run rig doctor to determine if your environment is set to run Outrigger.","title":"Run doctor"},{"location":"faq/troubleshooting/#ensure-the-environment-is-setup-correctly","text":"It can be useful to ensure everything is in a clean state. The following should ensure that rig stop stops the docker machine and cleans up networking eval \"$(rig config)\" clears environmental variables docker uses to communicate with the docker host rig start starts the docker machine eval \"$(rig config)\" sets environmental variables docker uses to communicate with the docker host","title":"Ensure the environment is setup correctly"},{"location":"faq/troubleshooting/#ensure-your-images-are-up-to-date","text":"From time to time images are updated to fix bugs or add functionality. You won't automatically receive these updates but you can fetch them when you hear new ones are available. docker pull imagename can be used if you want to update a specific image. For example, if you wanted to make sure you had the latest mariadb you'd run docker pull phase2/mariadb . docker-compose pull can be used within a project directory to make sure you've got the latest version of all images in the docker-compose.yml file.","title":"Ensure your images are up to date"},{"location":"faq/troubleshooting/#configure-your-shell","text":"If you do not have any containers listed when running docker ps or you get an error message like: Get http:///var/run/docker.sock/v1.20/containers/json: dial unix /var/run/docker.sock: no such file or directory. * Are you trying to connect to a TLS-enabled daemon without TLS? * Is your docker daemon up and running? Or an error message like: Couldn't connect to Docker daemon - you might need to run `boot2docker up`. Make sure your shell has the necessary environment variables by running: eval \"$(rig config)\"","title":"Configure Your Shell"},{"location":"faq/troubleshooting/#reset-everything","text":"If a problem continue to persists and the Docker Host is non-responsive, you may need to resort to the nuclear option of blowing everything away and starting over. Note this is a nuclear option as even your persistent data area will be removed if you don't back it up. If you have any data that needs to be maintained be sure to get a copy of it off of your VM first with the scripts provided. To wipe everything out and start over First backup your existing data (if desired) by running rig data-backup . This will sync your entire /data directory to your host machine. Then you can run rig remove This removes the broken Docker Host and it\u2019s state, making way for a clean start. Next you can rebuild everything by running rig start . This will create you new Docker Host. If you wish to restore the /data directory that you previously backed up, then run rig data-restore","title":"Reset everything"},{"location":"faq/troubleshooting/#files-not-found","text":"If you get messages about files not being found in the container that should be shared from your host computer, check the following: Is the project checked out under the /Users folder? Only files under the /Users folder are shared into the Docker Host (and thus containers) by default. Does the docker-compose.yml file have a volume mount set up that contains the missing files? Get shell in the Docker container via docker exec -it container name /bin/bash and check if the files are shared in the wrong place. Get shell in the Docker Machine with docker-machine ssh dev and check if you find the files under /Users.","title":"Files Not Found"},{"location":"faq/troubleshooting/#image-not-found","text":"If you encounter the following error about the Docker image not being found when starting a project, it may indicate that the image is private and your Docker client is not logged into a required private Docker Hub repository: Pulling repository phase2/privateimage Error: image phase2/privateimage:latest not found To solve this, run docker login and provide the relevant credentials.","title":"Image not found"},{"location":"faq/troubleshooting/#docker-client-and-server-version-incompatibilities","text":"If you see an error similar to this: Error response from daemon: client is newer than server (client API version: 1.20, server API version: 1.19) You likely need to upgrade your Docker Host to a get Docker client API compatibility. Do that with: `rig upgrade`","title":"Docker client and server version incompatibilities"},{"location":"faq/troubleshooting/#network-timed-out-and-cant-pull-container-image","text":"Example: Pulling cache (phase2/memcache:latest)... Pulling repository docker.io/phase2/memcache Network timed out while trying to connect to https://index.docker.io/v1/repositories/phase2/memcache/images. You may want to check your internet connection or if you are behind a proxy. Try restarting the Docker Machine: rig restart","title":"Network timed out and can't pull container image"},{"location":"faq/troubleshooting/#started-machines-may-have-a-new-ip-address","text":"Example: Started machines may have new IP addresses. You may need to re-run the `docker-machine env` command. [WARN] Docker daemon not running! Trying again in 3 seconds. Try 1 of 10. ... [WARN] Docker daemon not running! Trying again in 3 seconds. Try 10 of 10. [ERROR] Docker daemon failed to start! The Docker Host (probably called dev) has it's IP changed. The generated TLS Certs are no longer valid and must be regenerated. Possible causes or relations/patterns: Another VM was started in VirtualBox. Machine went to sleep and somehow caused issues with the running VM. Fix: Try running: docker-machine env If that does not work, you should be able to start the VM directly through docker-machine: docker-machine start dev Next, check the IP / TLS status by running: docker-machine ls The output will likely report something akin to: dev - virtualbox Running tcp://192.168.99.100:2376 Unknown Unable to query docker version: Get https://192.168.99.100:2376/v1.15/version: x509: certificate is valid for 192.168.99.101, not 192.168.99.100 Now, regenerate the TLS Certs: docker-machine regenerate-certs dev -f Devtools should now be able to start. Don't forget to run eval \"$(rig config)\" after.","title":"Started machines may have a new IP address"},{"location":"faq/troubleshooting/#containers-started-but-service-not-available","text":"Your Docker Host is running, the project's containers are up, and command-line operations work fine. Why can't you view the site in your web browser?","title":"Containers Started but Service Not Available"},{"location":"faq/troubleshooting/#dns-services","text":"The DNS services that Outrigger spins up may not be working. Run docker ps and see that you have a dnsdock and dnsmasq container. If those services are not running, try rig dns to bring them up, or a full rig restart if that does not work. rig dns-records is also useful to see what containers have registered names.","title":"DNS Services"},{"location":"faq/troubleshooting/#dns-configuration","text":"It is also possible that the DNS services are running for your environment, but somehow the configuration is wrong. Run rig dns-records and make sure your project has an entry. If not, you may need another restart, or perhaps you are missing com.dnsdock.name and com.dnsdock.image labels in your docker-compose.yml?","title":"DNS Configuration"},{"location":"faq/troubleshooting/#slow-starting-services","text":"Some services, such as Apache Solr or Varnish, can take longer to start up than Apache and PHP-FPM. As a result you might load the browser so fast that not all services are available, which in the case of a proxy may prevent the page from loading at all. Wait a short time and try reloading the page.","title":"Slow-starting Services"},{"location":"faq/troubleshooting/#failed-health-checks","text":"Some services such as Varnish depend on others to operate, and have built-in health checks to verify the other service is operating. If such a health check fails, there could be two problems: The internal DNS routing between Docker containers is broken. Make sure the configuration of your services is correct. The dependency (e.g., Apache behind Varnish) is not yet up and running when Varnish performs its checks. In either case, you can often repair the problem by performing a clean restart of the broken service. docker-compose stop proxy docker-compose rm -f proxy docker-compose up -d proxy Your other services should already be up and functional when this is done, so the health check will not fail on account of (2). Checking on Varnish Health If you suspect Varnish may be failing, run docker exec -it [VARNISH_CONTAINER] varnishlog and scan for VCL compilation errors.","title":"Failed Health Checks"},{"location":"faq/troubleshooting/#service-became-non-responsive","text":"Sometimes a service locks up. Apache stops serving results, Solr stops indexing or responding to search queries. These things happen on servers all the time. It may even happen more often on Docker, especially since we are now using more \"infrastructure\" in our local environments. Docker is meant to easily sandbox these problems from the rest of your machine, and to easily resolve these problems by allowing you to dump the problem and start over fresh very easily. Hard reset on a service (as describe above), is very much like a power cycle: docker-compose stop [BROKEN_SERVICE] docker-compose rm -f [BROKEN_SERVICE] docker-compose up -d [BROKEN_SERVICE] Sometimes data for a service is volume mounted from outside the container. This persistence is good when the data is healthy, but can be really bad if the data is part of the problem (e.g., broken lockfiles). docker-machine ssh [MACHINE_NAME] rm -Rf /data/[PROJECT]/[DIRECTORY_FOR_SERVICE] Remember that your service may have configuration or data in another service, you may need to perform this operation against multiple containers (e.g., Solr)","title":"Service Became Non-Responsive"},{"location":"faq/troubleshooting/#nfs-conflicts-with-other-environments","text":"Running the Devtools VM in conjunction with other development environments (such as Vagrant) can sometimes cause conflicts with volume mounts. This is due to the fact that the Devtools VM mounts the entirety of /Users into the VM on OS X. Additional NFS mounts from other environments that target subdirectories of /Users will fail. The easiest workaround is to keep projects that use non-Devtools environments like Vagrant in a directory outside of /Users , such as /opt . Alternatively, you can run a single VM at a time and manually clear out /etc/exports prior to switching environments/projects.","title":"NFS Conflicts with other Environments"},{"location":"faq/troubleshooting/#migrating-vagrant-boxes-outside-of-users","text":"If you'd like to move your vagrant boxes outside of your home directory, perform the following steps: Choose a destination folder. We'll be using /opt/vms for this example. Insure the destination has enough free space. Typically this is not a problem because our Macs are one single partition. Make the new directory, and ensure your userid owns it: sudo mkdir /opt/vms; sudo chown -R userid:userid /opt/vms Add export VAGRANT_HOME=/opt/vms to ~/.bash_profile Move your vagrant folders over to /opt/vms. edit /etc/exports, updating any vagrant mount point to use the new location. (example: /Users/userid/vagrant/projectx/cms) While a reboot isn't necessary, it'll help to make sure nothing is running and using the old mount points. cd into your new vagrant directory, and do a vagrant up.","title":"Migrating Vagrant boxes outside of /Users"},{"location":"faq/troubleshooting/#permission-errors-during-container-startup","text":"NFS mounts from the host volume into a container may require a specific UID mapping in order for proper permission setup within the container. SSH key volume mounts are common in project build containers. If there's a problem with permissions for host files or directories being mounted inside the docker-machine via NFS, it could be a UID mapping problem. One way this problem manifests itself is, given a mount entry in a .yml file like: volumes: - ~/.ssh/id_rsa:/root/.ssh/id_rsa When starting the container, you may see an error like: ERROR: Cannot start service cli: error while creating mount source path '/Users/username/.ssh/id_rsa': mkdir /Users/username/.ssh/id_rsa: permission denied Verify that the UID of the OSX directory is mapped correctly in /etc/exports . In the host operating system (example is for OSX): $ id uid=502(username) gid=20(staff) groups=20(staff),... In /etc/exports: # docker-machine-nfs-begin machine-name /Users 192.168.99.100 -alldirs -mapall=501:20 # -- incorrect # docker-machine-nfs-end machine-name The -mapall arguments need to match your uid and gid from the id command. If you need to change /etc/exports (in the host), edit it, save the changes, then restart the nfsd : rig stop sudo nfsd restart rig start","title":"Permission Errors During Container Startup"},{"location":"faq/troubleshooting/#failures-during-project-sync-startup","text":"The initial sync process involves scanning your codebase to assemble a list of all the files it contains. For larger codebases or slower computers this process may not complete before a timeout is reached. The timeout attempts to ensure that if there is a problem that prevents the unison container or local process from starting you don't wait without realizing there is an issue. To see if you are getting an erroneous message message you can look to see if a local unison process and a unison container for your project eventually start. The easiest check can be to run the rig project sync command with the initial-sync-timeout option set to a large value such as 300 seconds. If you want to dig deeper, look for a local unison process on your host machine with your project sync volume name as a log file. For example: ps aux | grep myproject-sync.log . There should also be a docker container running with a name like myproject-sync. If both of those are present it is possible that a timeout that is too short is affecting you.","title":"Failures during project sync startup"},{"location":"faq/troubleshooting/#identifying-and-troubleshooting-sync-issues","text":"When the unison filesystem based syncing stops working it unfortunately does so silently. If the files your container is using don't seem to be updating when you think they should it may be that the syncing has stopped working. This has typically been observed happening when putting the host machine into sleep mode. It may also happen if something causes either side of the syncing process to crash due to a very high rate of filesystem changes. Simply restarting the sync usually resolves the issue. As of version 2.2.0, Outrigger has commands to help determine the state of the filesystem syncing a as well as commands that facilitate scripting and cleanup. See the Additional Sync Commands section of Filesystem Sync for information about these commands.","title":"Identifying and troubleshooting sync issues"},{"location":"getting-started/first-steps/","text":"First Steps To see a list of the commands that rig provides run rig --help . rig often offers two levels of commands which allows for grouping all actions related to a similar concept together. This is most evident with the commands related to a project. To see detailed help about a particular command, including any available subcommands, run rig [command] --help . For example, in a project directory, running rig project --help will show you a list of the project specific actions available to you, as well as other project related subcommands such as create and sync . To see detailed help about a particular subcommand, run rig [command] [subcommand] --help , for example rig project create --help .","title":"First Steps"},{"location":"getting-started/first-steps/#first-steps","text":"To see a list of the commands that rig provides run rig --help . rig often offers two levels of commands which allows for grouping all actions related to a similar concept together. This is most evident with the commands related to a project. To see detailed help about a particular command, including any available subcommands, run rig [command] --help . For example, in a project directory, running rig project --help will show you a list of the project specific actions available to you, as well as other project related subcommands such as create and sync . To see detailed help about a particular subcommand, run rig [command] [subcommand] --help , for example rig project create --help .","title":"First Steps"},{"location":"getting-started/linux-installation/","text":"Linux Installation When running Docker containers on Linux, it is not necessary to run the Docker Machine VM. The instructions here describe how to run Outrigger projects on Linux. Install Outrigger Go to the Outrigger Releases page and download the .rpm , .deb , or grab the linux-amd64.tar.gz binary directly and install it in /usr/local/bin Linux Requirements For automated DNS resolution, the use of one of two options to resolve DNS queries to the dnsdock container Linux installation on Fedora/Centos Install Docker for Fedora or Install Docker for CentOS Install Docker Compose Set the DNS configuration for Docker We need to add Docker daemon configuration sudo vi /etc/docker/daemon.conf In that file put (or integrate) the following: { dns : [ 172.17.0.1 ] } Set up the docker0 network as trusted sudo firewall-cmd --zone=trusted --add-interface=docker0 sudo firewall-cmd --zone=trusted --add-interface=docker0 --permanent Restart the docker daemon sudo systemctl restart docker Linux installation on Ubuntu/Debian Install Docker for Ubuntu or Install Docker for Debian Install Docker Compose Set the DNS configuration for Docker We need to add Docker daemon configuration sudo vi /etc/docker/daemon.conf In that file put (or integrate) the following: { dns : [ 172.17.0.1 ] } Restart the docker daemon sudo systemctl restart docker Automated Linux DNS configuration options Outrigger will help automate the setup of DNS (via rig start and rig dns ) if you are using one of the following options for name resolution dnsmasq via NetworkManager libnss-resolver Manual DNS resolution options DNSDock as main resolver This method will probably only work well if this is a fixed computer or server with a consistent single upstream DNS server. If you meet these criteria, you can very easily use this to set up .vm resolution for containers an delegate the rest to your normal DNS server. This example assumes that the upstream DNS server for a Linux workstation is 192.168.0.1. Run the dnsdock container, specifying your upstream DNS server at the end. docker run -d \\ --name=dnsdock \\ --restart=always \\ -l com.dnsdock.name=dnsdock \\ -l com.dnsdock.image=outrigger \\ -p 172.17.0.1:53:53/udp \\ -v /var/run/docker.sock:/var/run/docker.sock \\ aacebedo/dnsdock:v1.16.4-amd64 --domain=vm Configure 172.17.0.1 as your first DNS resolver in your network configuration. The method for doing this may differ based on whether you are using a desktop environment or running Linux on a server, but that nameserver should end up as the first 'nameserver' line in your /etc/resolv.conf file. Running dnsdock as a service Create the file /etc/systemd/system/dnsdock.service with the following contents [Unit] Description=DNSDock After=docker.service Requires=docker.service [Service] TimeoutStartSec=0 ExecStartPre=-/usr/bin/docker kill dnsdock ExecStartPre=-/usr/bin/docker rm dnsdock ExecStart=/usr/bin/docker run --rm --name dnsdock -v /var/run/docker.sock:/var/run/docker.sock -l com.dnsdock.name=dnsdock -l com.dnsdock.image=outrigger -p 172.17.0.1:53:53/udp aacebedo/dnsdock:v1.16.4-amd64 --domain=vm ExecStop=/usr/bin/docker stop dnsdock Restart=always RestartSec=30 [Install] WantedBy=multi-user.target Ensure Docker is registered with systemctl systemctl enable docker Register dnsdock service: systemctl enable dnsdock systemctl daemon-reload Now you can start/stop the dnsdock service with systemctl [start|stop] dnsdock You can also check its status systemctl status dnsdock Verifying DNS is working Once you have your environment set up, you can use the following tests to ensure things are running properly. dig @172.17.0.1 dnsdock.outrigger.vm. You should get a 172.17.0.0/16 address ping dnsdock.outrigger.vm You should get echo replies from a 172.17.0.0/16 address getent hosts dnsdock.outrigger.vm You should get a 172.17.0.0/16 address Verify DNS works between containers Open a shell for a second test container docker run --rm -l com.dnsdock.name=test -l com.dnsdock.image=outrigger -it alpine sh From its prompt: ping test.outrigger.vm You should get echo replies from a 172.17.0.0/16 address Troubleshooting bind: address already in use Make sure you are not running a service which binds all mapped IPs. For example, docker run -d \\ --name=dnsdock \\ --restart=always \\ -l com.dnsdock.name=dnsdock \\ -l com.dnsdock.image=outrigger \\ -p 172.17.0.1:53:53/udp \\ -v /var/run/docker.sock:/var/run/docker.sock \\ aacebedo/dnsdock:v1.16.4-amd64 --domain=vm 6aa76d0df98ede7e01c1cef53f105a79a60bab72ee905a1acd76ad57d4aeb014 docker: Error response from daemon: driver failed programming external connectivity on endpoint dnsdock (62b3788e1f60530d1f468b46833f0bf8227955557da3a135356184f5b7d57bde): Error starting userland proxy: listen udp 172.17.0.1:53: bind: address already in use. In this case the culprit was bind9/named was attaching to all interfaces (on port 53) The solution systemctl stop bind9 To avoid having to stop bind9 every session systemctl disable bind9 You may need to restart NetworkManager systemctl restart NetworkManager","title":"Linux Installation"},{"location":"getting-started/linux-installation/#linux-installation","text":"When running Docker containers on Linux, it is not necessary to run the Docker Machine VM. The instructions here describe how to run Outrigger projects on Linux.","title":"Linux Installation"},{"location":"getting-started/linux-installation/#install-outrigger","text":"Go to the Outrigger Releases page and download the .rpm , .deb , or grab the linux-amd64.tar.gz binary directly and install it in /usr/local/bin","title":"Install Outrigger"},{"location":"getting-started/linux-installation/#linux-requirements","text":"For automated DNS resolution, the use of one of two options to resolve DNS queries to the dnsdock container","title":"Linux Requirements"},{"location":"getting-started/linux-installation/#linux-installation-on-fedoracentos","text":"Install Docker for Fedora or Install Docker for CentOS Install Docker Compose Set the DNS configuration for Docker We need to add Docker daemon configuration sudo vi /etc/docker/daemon.conf In that file put (or integrate) the following: { dns : [ 172.17.0.1 ] } Set up the docker0 network as trusted sudo firewall-cmd --zone=trusted --add-interface=docker0 sudo firewall-cmd --zone=trusted --add-interface=docker0 --permanent Restart the docker daemon sudo systemctl restart docker","title":"Linux installation on Fedora/Centos"},{"location":"getting-started/linux-installation/#linux-installation-on-ubuntudebian","text":"Install Docker for Ubuntu or Install Docker for Debian Install Docker Compose Set the DNS configuration for Docker We need to add Docker daemon configuration sudo vi /etc/docker/daemon.conf In that file put (or integrate) the following: { dns : [ 172.17.0.1 ] } Restart the docker daemon sudo systemctl restart docker","title":"Linux installation on Ubuntu/Debian"},{"location":"getting-started/linux-installation/#automated-linux-dns-configuration-options","text":"Outrigger will help automate the setup of DNS (via rig start and rig dns ) if you are using one of the following options for name resolution dnsmasq via NetworkManager libnss-resolver","title":"Automated Linux DNS configuration options"},{"location":"getting-started/linux-installation/#manual-dns-resolution-options","text":"","title":"Manual DNS resolution options"},{"location":"getting-started/linux-installation/#dnsdock-as-main-resolver","text":"This method will probably only work well if this is a fixed computer or server with a consistent single upstream DNS server. If you meet these criteria, you can very easily use this to set up .vm resolution for containers an delegate the rest to your normal DNS server. This example assumes that the upstream DNS server for a Linux workstation is 192.168.0.1. Run the dnsdock container, specifying your upstream DNS server at the end. docker run -d \\ --name=dnsdock \\ --restart=always \\ -l com.dnsdock.name=dnsdock \\ -l com.dnsdock.image=outrigger \\ -p 172.17.0.1:53:53/udp \\ -v /var/run/docker.sock:/var/run/docker.sock \\ aacebedo/dnsdock:v1.16.4-amd64 --domain=vm Configure 172.17.0.1 as your first DNS resolver in your network configuration. The method for doing this may differ based on whether you are using a desktop environment or running Linux on a server, but that nameserver should end up as the first 'nameserver' line in your /etc/resolv.conf file. Running dnsdock as a service Create the file /etc/systemd/system/dnsdock.service with the following contents [Unit] Description=DNSDock After=docker.service Requires=docker.service [Service] TimeoutStartSec=0 ExecStartPre=-/usr/bin/docker kill dnsdock ExecStartPre=-/usr/bin/docker rm dnsdock ExecStart=/usr/bin/docker run --rm --name dnsdock -v /var/run/docker.sock:/var/run/docker.sock -l com.dnsdock.name=dnsdock -l com.dnsdock.image=outrigger -p 172.17.0.1:53:53/udp aacebedo/dnsdock:v1.16.4-amd64 --domain=vm ExecStop=/usr/bin/docker stop dnsdock Restart=always RestartSec=30 [Install] WantedBy=multi-user.target Ensure Docker is registered with systemctl systemctl enable docker Register dnsdock service: systemctl enable dnsdock systemctl daemon-reload Now you can start/stop the dnsdock service with systemctl [start|stop] dnsdock You can also check its status systemctl status dnsdock","title":"DNSDock as main resolver"},{"location":"getting-started/linux-installation/#verifying-dns-is-working","text":"Once you have your environment set up, you can use the following tests to ensure things are running properly. dig @172.17.0.1 dnsdock.outrigger.vm. You should get a 172.17.0.0/16 address ping dnsdock.outrigger.vm You should get echo replies from a 172.17.0.0/16 address getent hosts dnsdock.outrigger.vm You should get a 172.17.0.0/16 address Verify DNS works between containers Open a shell for a second test container docker run --rm -l com.dnsdock.name=test -l com.dnsdock.image=outrigger -it alpine sh From its prompt: ping test.outrigger.vm You should get echo replies from a 172.17.0.0/16 address","title":"Verifying DNS is working"},{"location":"getting-started/linux-installation/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"getting-started/linux-installation/#bind-address-already-in-use","text":"Make sure you are not running a service which binds all mapped IPs. For example, docker run -d \\ --name=dnsdock \\ --restart=always \\ -l com.dnsdock.name=dnsdock \\ -l com.dnsdock.image=outrigger \\ -p 172.17.0.1:53:53/udp \\ -v /var/run/docker.sock:/var/run/docker.sock \\ aacebedo/dnsdock:v1.16.4-amd64 --domain=vm 6aa76d0df98ede7e01c1cef53f105a79a60bab72ee905a1acd76ad57d4aeb014 docker: Error response from daemon: driver failed programming external connectivity on endpoint dnsdock (62b3788e1f60530d1f468b46833f0bf8227955557da3a135356184f5b7d57bde): Error starting userland proxy: listen udp 172.17.0.1:53: bind: address already in use. In this case the culprit was bind9/named was attaching to all interfaces (on port 53) The solution systemctl stop bind9 To avoid having to stop bind9 every session systemctl disable bind9 You may need to restart NetworkManager systemctl restart NetworkManager","title":"bind: address already in use"},{"location":"getting-started/mac-installation/","text":"Installation Mac Installation Install VirtualBox VirtualBox Downloads Install Homebrew Homebrew Website If Homebrew is already installed, then be sure to do a brew update Tap the Outrigger repository brew tap phase2/outrigger Install Outrigger (and dependencies) brew install outrigger-cli This will install the rig binary as well the Docker and other dependencies. Create the Docker Host Once everything checks out, run the following command to create a new docker host. (You will likely be prompted for your admin password) rig start Here are some configuration options available to you to customize your setup: Options on rig : name: The Docker Machine name for the VM. Defaults to dev Options on the start command: --driver: The driver to create the Docker Machine with. Choices are: virtualbox - default vmwarefusion xhyve --cpu-count The number of virtual CPU you want to allocate to this VM. Defaults to 2 --memory-size The size memory you want to configure for this VM, in Megabytes. Defaults to 4096 --disk-size The size drive you want to configure for this VM, in Gigabytes. Defaults to 40 Configure your shell to set Outrigger Docker environment To configure the shell with the proper Outrigger environment, run the following command after the docker host has started from the previous step. eval \"$(rig config)\" For convenience, you should make this automatic on every terminal you launch. To do that add the following to your .bash_profile , .zshrc or equivalent: # Support for Outrigger eval $(rig config) alias re='eval $(rig config) ' Running rig config Even with automatic execution in your shell, this command must be run in any existing terminal windows after rig start or rig restart commands. To support that see the re alias in the shell init file above. Just run re after a rig start Support for Docker for Mac See Docker for Mac Support","title":"Mac Installation"},{"location":"getting-started/mac-installation/#installation","text":"","title":"Installation"},{"location":"getting-started/mac-installation/#mac-installation","text":"","title":"Mac Installation"},{"location":"getting-started/mac-installation/#install-virtualbox","text":"VirtualBox Downloads","title":"Install VirtualBox"},{"location":"getting-started/mac-installation/#install-homebrew","text":"Homebrew Website If Homebrew is already installed, then be sure to do a brew update","title":"Install Homebrew"},{"location":"getting-started/mac-installation/#tap-the-outrigger-repository","text":"brew tap phase2/outrigger","title":"Tap the Outrigger repository"},{"location":"getting-started/mac-installation/#install-outrigger-and-dependencies","text":"brew install outrigger-cli This will install the rig binary as well the Docker and other dependencies.","title":"Install Outrigger (and dependencies)"},{"location":"getting-started/mac-installation/#create-the-docker-host","text":"Once everything checks out, run the following command to create a new docker host. (You will likely be prompted for your admin password) rig start Here are some configuration options available to you to customize your setup: Options on rig : name: The Docker Machine name for the VM. Defaults to dev Options on the start command: --driver: The driver to create the Docker Machine with. Choices are: virtualbox - default vmwarefusion xhyve --cpu-count The number of virtual CPU you want to allocate to this VM. Defaults to 2 --memory-size The size memory you want to configure for this VM, in Megabytes. Defaults to 4096 --disk-size The size drive you want to configure for this VM, in Gigabytes. Defaults to 40","title":"Create the Docker Host"},{"location":"getting-started/mac-installation/#configure-your-shell-to-set-outrigger-docker-environment","text":"To configure the shell with the proper Outrigger environment, run the following command after the docker host has started from the previous step. eval \"$(rig config)\" For convenience, you should make this automatic on every terminal you launch. To do that add the following to your .bash_profile , .zshrc or equivalent: # Support for Outrigger eval $(rig config) alias re='eval $(rig config) ' Running rig config Even with automatic execution in your shell, this command must be run in any existing terminal windows after rig start or rig restart commands. To support that see the re alias in the shell init file above. Just run re after a rig start","title":"Configure your shell to set Outrigger Docker environment"},{"location":"getting-started/mac-installation/#support-for-docker-for-mac","text":"See Docker for Mac Support","title":"Support for Docker for Mac"},{"location":"getting-started/system-requirements/","text":"System Requirements Hardware We recommend 16 GB of RAM when working with containerization / virtualization. You can certainly get by with 8 GB but you may want to keep other applications to a minimum. It is also dependent on the RAM configuration required if using virtualization. By default the Outrigger Docker Host is configured to run with 4 GB of RAM. Software Mac Homebrew is also required for package installation. For Virtualization you can choose one of the following VirtualBox 5.1.x (the default supported option) VMWare Fusion Xhyve with Docker Machine Xhyve Windows VirtualBox is required. Linux Docker is required to be running as a system service.","title":"System Requirements"},{"location":"getting-started/system-requirements/#system-requirements","text":"","title":"System Requirements"},{"location":"getting-started/system-requirements/#hardware","text":"We recommend 16 GB of RAM when working with containerization / virtualization. You can certainly get by with 8 GB but you may want to keep other applications to a minimum. It is also dependent on the RAM configuration required if using virtualization. By default the Outrigger Docker Host is configured to run with 4 GB of RAM.","title":"Hardware"},{"location":"getting-started/system-requirements/#software","text":"","title":"Software"},{"location":"getting-started/system-requirements/#mac","text":"Homebrew is also required for package installation. For Virtualization you can choose one of the following VirtualBox 5.1.x (the default supported option) VMWare Fusion Xhyve with Docker Machine Xhyve","title":"Mac"},{"location":"getting-started/system-requirements/#windows","text":"VirtualBox is required.","title":"Windows"},{"location":"getting-started/system-requirements/#linux","text":"Docker is required to be running as a system service.","title":"Linux"},{"location":"getting-started/windows-installation/","text":"Windows Installation Warning Windows is not yet fully supported but the following information should help you get a functioning environment running. Submissions of information that helps improve the command examples and that can be rolled into the automation afforded by the rig binary are welcome. Install VirtualBox VirtualBox Downloads Install docker tools You will need to install the following docker tools. It is recommended that you use chocolatey or an installation mechanism which allows you to simply acquire these binaries with no additional setup. docker docker-machine docker-compose Download the latest rig binary The latest Windows binary for rig can be found at https://github.com/phase2/rig/releases. This binary is responsible for a few tasks on the initial execution of the rig start command. Information below assumes default values for all options. Checking for the existence of a virtual machine instance to serve as a docker host and creation if it does not exist. Creation is of a virtual machine named dev using a boot2docker iso file based on the version of the docker binary installed. This is a command that typically looks like: docker-machine create dev --driver=virtualbox --virtualbox-boot2docker-url=https://github.com/boot2docker/boot2docker/releases/download/v17.04.0-ce-rc1/boot2docker.iso --virtualbox-memory=4096 --virtualbox-cpu-count=2 --virtualbox-disk-size=40000 --engine-opt dns=172.17.42.1 --engine-opt bip=172.17.42.1/16 --virtualbox-hostonly-nicpromisc allow-all Startup of created virtual machine serving as docker host. Ensuring proper environmental variable settings for communicating with the running machine. See docker-machine env dev for needed variables and values. Setup of routing rules so that all traffic destined for 172.17.0.0/16 is routed to the IP address of the running instance. This is usually a command that looks something like: runas /noprofile /user:Administrator route DELETE 172.17.0.0 to clean any lingering setup and then runas /noprofile /user:Administrator route -p ADD 172.17.0.0/16 $(docker-machine ip dev) Setting up the /data directory within the docker host to point at a persistent disk to allow space for non ephemeral data to be stored. There should be a /mnt/sda1 drive available within the virtual machine that a data directory should be created on and then linked to via the command sudo ln -s /mnt/sda1/data /data Instantiation of a dnsdock container to provide DNS services via the command docker run --restart=always -d -v /var/run/docker.sock:/var/run/docker.sock -e DNSDOCK_NAME=dnsdock -e DNSDOCK_IMAGE=dnsdock --name dnsdock -p 172.17.42.1:53:53/udp phase2/dnsdock Setup of DNS services so that *.vm addresses attempt to resolve against the running dnsdock container. It is currently unknown how to get Windows to accomplish this in a safe way when the containers are not running but a work around is to use the rig dns-records command to provide output for entry into the C:\\Windows\\System32\\Drivers\\etc\\hosts file. Sharing of the home directory of all of your users via NFS with the dev instance. On OS X this is accomplished via the docker-machine-nfs.sh script. This will allow for code on your local machine to be executed from within a running container as long as it is in your home directory.","title":"Windows Installation"},{"location":"getting-started/windows-installation/#windows-installation","text":"Warning Windows is not yet fully supported but the following information should help you get a functioning environment running. Submissions of information that helps improve the command examples and that can be rolled into the automation afforded by the rig binary are welcome.","title":"Windows Installation"},{"location":"getting-started/windows-installation/#install-virtualbox","text":"VirtualBox Downloads","title":"Install VirtualBox"},{"location":"getting-started/windows-installation/#install-docker-tools","text":"You will need to install the following docker tools. It is recommended that you use chocolatey or an installation mechanism which allows you to simply acquire these binaries with no additional setup. docker docker-machine docker-compose","title":"Install docker tools"},{"location":"getting-started/windows-installation/#download-the-latest-rig-binary","text":"The latest Windows binary for rig can be found at https://github.com/phase2/rig/releases. This binary is responsible for a few tasks on the initial execution of the rig start command. Information below assumes default values for all options. Checking for the existence of a virtual machine instance to serve as a docker host and creation if it does not exist. Creation is of a virtual machine named dev using a boot2docker iso file based on the version of the docker binary installed. This is a command that typically looks like: docker-machine create dev --driver=virtualbox --virtualbox-boot2docker-url=https://github.com/boot2docker/boot2docker/releases/download/v17.04.0-ce-rc1/boot2docker.iso --virtualbox-memory=4096 --virtualbox-cpu-count=2 --virtualbox-disk-size=40000 --engine-opt dns=172.17.42.1 --engine-opt bip=172.17.42.1/16 --virtualbox-hostonly-nicpromisc allow-all Startup of created virtual machine serving as docker host. Ensuring proper environmental variable settings for communicating with the running machine. See docker-machine env dev for needed variables and values. Setup of routing rules so that all traffic destined for 172.17.0.0/16 is routed to the IP address of the running instance. This is usually a command that looks something like: runas /noprofile /user:Administrator route DELETE 172.17.0.0 to clean any lingering setup and then runas /noprofile /user:Administrator route -p ADD 172.17.0.0/16 $(docker-machine ip dev) Setting up the /data directory within the docker host to point at a persistent disk to allow space for non ephemeral data to be stored. There should be a /mnt/sda1 drive available within the virtual machine that a data directory should be created on and then linked to via the command sudo ln -s /mnt/sda1/data /data Instantiation of a dnsdock container to provide DNS services via the command docker run --restart=always -d -v /var/run/docker.sock:/var/run/docker.sock -e DNSDOCK_NAME=dnsdock -e DNSDOCK_IMAGE=dnsdock --name dnsdock -p 172.17.42.1:53:53/udp phase2/dnsdock Setup of DNS services so that *.vm addresses attempt to resolve against the running dnsdock container. It is currently unknown how to get Windows to accomplish this in a safe way when the containers are not running but a work around is to use the rig dns-records command to provide output for entry into the C:\\Windows\\System32\\Drivers\\etc\\hosts file. Sharing of the home directory of all of your users via NFS with the dev instance. On OS X this is accomplished via the docker-machine-nfs.sh script. This will allow for code on your local machine to be executed from within a running container as long as it is in your home directory.","title":"Download the latest rig binary"},{"location":"project-setup/docker-images/","text":"Docker Images Key components The following Docker Images have been built to work with Outrigger. They all have a similar and consistent setup, so when using these images it is important to know the technology in place and how to customize it for you purposes. We have provided environmental configuration for the most frequently customized options, and any extended customization can be made by following the recommendations in Customizing container configuration These images are built with the following software S6-overlay Init System confd config file templating Note Below are a collection of tuned images for working with Outrigger, but any Docker Image can be used within Outrigger. Additionally these Docker Images do not need to be used with Outrigger, they can be used in any Docker environment. Images outrigger/servicebase ( Docker Hub ) ( Repo ) A CentOS 7 base image that has s6-overlay and confd. This is a useful image for extending to build non-trivial services. outrigger/servicebaselight ( Docker Hub ) ( Repo ) This is an Alpine-based image that has had the S6-overlay init system and confd added to it. In addition to the lightweight Alpine base it also includes bash and glibc so that Go-based Linux binaries will run. This image is only ~8MB when compared to the 100+MB of servicebase. outrigger/apache-php ( Docker Hub ) ( Repo ) Apache PHP runtime images. It currently support PHP 5.5, 5.6, and 7.0 outrigger/apache-php-base ( Docker Hub ) ( Repo ) A base image for outrigger/apache-php. Includes Apache and a default VirtualHost configured with a proxy to PHP-FPM. It does not include the php runtime, that is handed in the extension image(s). outrigger/build ( Docker Hub ) ( Repo ) This image provides the collection of development tools necessary to build applications, bundled with a wide array of tools useful for development and troubleshooting via the command-line interface. While it is possible to directly connect via the \"web\" containers (apache-php), this is the preferred way to perform \"server work\". It also contains some extras you may need to work with Drupal, including use of tools such as Drupal Console , Grunt Drupal Tasks and Pattern Lab Starter . outrigger/mariadb ( Docker Hub ) ( Repo ) MariaDB image for MySQL based builds with confd templates for config outrigger/memcache ( Docker Hub ) ( Repo ) Memcache image with configurable settings outrigger/redis ( Docker Hub ) ( Repo ) Redis image with a confd template for redis.conf outrigger/node ( Docker Hub ) ( Repo ) Node image outrigger/varnish ( Docker Hub ) ( Repo ) Varnish container with fancy environment variables for easy configuration outrigger/jenkins-docker ( Docker Hub ) ( Repo ) Jenkins image that is built to be able to run Docker commands and launch containers. Docker-inception.","title":"Docker Images"},{"location":"project-setup/docker-images/#docker-images","text":"","title":"Docker Images"},{"location":"project-setup/docker-images/#key-components","text":"The following Docker Images have been built to work with Outrigger. They all have a similar and consistent setup, so when using these images it is important to know the technology in place and how to customize it for you purposes. We have provided environmental configuration for the most frequently customized options, and any extended customization can be made by following the recommendations in Customizing container configuration These images are built with the following software S6-overlay Init System confd config file templating Note Below are a collection of tuned images for working with Outrigger, but any Docker Image can be used within Outrigger. Additionally these Docker Images do not need to be used with Outrigger, they can be used in any Docker environment.","title":"Key components"},{"location":"project-setup/docker-images/#images","text":"","title":"Images"},{"location":"project-setup/docker-images/#outriggerservicebase","text":"( Docker Hub ) ( Repo ) A CentOS 7 base image that has s6-overlay and confd. This is a useful image for extending to build non-trivial services.","title":"outrigger/servicebase"},{"location":"project-setup/docker-images/#outriggerservicebaselight","text":"( Docker Hub ) ( Repo ) This is an Alpine-based image that has had the S6-overlay init system and confd added to it. In addition to the lightweight Alpine base it also includes bash and glibc so that Go-based Linux binaries will run. This image is only ~8MB when compared to the 100+MB of servicebase.","title":"outrigger/servicebaselight"},{"location":"project-setup/docker-images/#outriggerapache-php","text":"( Docker Hub ) ( Repo ) Apache PHP runtime images. It currently support PHP 5.5, 5.6, and 7.0","title":"outrigger/apache-php"},{"location":"project-setup/docker-images/#outriggerapache-php-base","text":"( Docker Hub ) ( Repo ) A base image for outrigger/apache-php. Includes Apache and a default VirtualHost configured with a proxy to PHP-FPM. It does not include the php runtime, that is handed in the extension image(s).","title":"outrigger/apache-php-base"},{"location":"project-setup/docker-images/#outriggerbuild","text":"( Docker Hub ) ( Repo ) This image provides the collection of development tools necessary to build applications, bundled with a wide array of tools useful for development and troubleshooting via the command-line interface. While it is possible to directly connect via the \"web\" containers (apache-php), this is the preferred way to perform \"server work\". It also contains some extras you may need to work with Drupal, including use of tools such as Drupal Console , Grunt Drupal Tasks and Pattern Lab Starter .","title":"outrigger/build"},{"location":"project-setup/docker-images/#outriggermariadb","text":"( Docker Hub ) ( Repo ) MariaDB image for MySQL based builds with confd templates for config","title":"outrigger/mariadb"},{"location":"project-setup/docker-images/#outriggermemcache","text":"( Docker Hub ) ( Repo ) Memcache image with configurable settings","title":"outrigger/memcache"},{"location":"project-setup/docker-images/#outriggerredis","text":"( Docker Hub ) ( Repo ) Redis image with a confd template for redis.conf","title":"outrigger/redis"},{"location":"project-setup/docker-images/#outriggernode","text":"( Docker Hub ) ( Repo ) Node image","title":"outrigger/node"},{"location":"project-setup/docker-images/#outriggervarnish","text":"( Docker Hub ) ( Repo ) Varnish container with fancy environment variables for easy configuration","title":"outrigger/varnish"},{"location":"project-setup/docker-images/#outriggerjenkins-docker","text":"( Docker Hub ) ( Repo ) Jenkins image that is built to be able to run Docker commands and launch containers. Docker-inception.","title":"outrigger/jenkins-docker"},{"location":"project-setup/existing-projects/","text":"Existing Projects To add Outrigger support to an existing project, you'll need to understand how to create: Docker Compose file(s) Custom Domain Names for Project Services Local code sharing with your containers Persistent data storage An ( optional ) .outrigger.yml file Docker Compose file(s) Outrigger is intended to work with either raw Docker commands or Docker Compose files, and we find things are best when orchestrating your local environments using Docker Compose. Be sure to read the Docker Compose compose file documentation so that you have a handle on how to setup the needed configuration. Define Project Services The essential point is that you will need to define a Docker Compose Service for each of the components in your architecture. Web/API Server, database, cache/key-value store, search server, proxy cache, mail server, etc. Each of those will be configured as a service in your docker-compose.yml file. Please see the Outrigger Examples Repository to see setups of various technologies within Outrigger. Specifically, the Drupal 8 example covers all of the high points. Build Container In addition to containers for all of your project services, we find it useful to have a container we refer to as the build container. This container provides the tools supporting development of your project. The default build image for Outrigger is outrigger/build . It includes tools supporting a wide variety of modern web development such as node , npm , composer , grunt , gulp , bower , etc. We often configure the build container and associated command containers in a file named build.yml . The Drupal 8 example mentioned before contains a sample a build.yml file. Choosing Images For Project Services Any valid Docker Image can work with Outrigger. We also supply a collection of crafted Docker images that are easily configured and optimized. An example for an Apache / PHP image based on PHP7 is outrigger/apache-php:php70 or php:7.1.1-apache Use Image Tags When selecting and image to use, be sure to also specify a tag for that image. When you do not specify a tag, the default tag of latest is used and it is easy to get out of sync with other team members or simply get unwanted changes if you do not explicitly specify an image tag. General Config Once you set up your docker-compose.yml file, you'll likely need to customize your services for a variety of reasons. Many images will use environment variables and/or volume mounts to customize the running container configuration. See Customizing Container Configuration to learn more about customizing your configuration for your project. Custom Domain Names for Project Services See DNS Resolution to learn how to use Docker Labels to specify custom domain names for your project's services. Sharing code into and across containers The straightforward way to get code from your host into containers is to use an NFS volume mount. This allows you, for example, to mount the current directory ( . ) to a specific place inside a container ( /var/www ) with a simple volume mount specification in your docker-compose.yml file. See the Volume Mount documentation on volume mount specifications. For example: .:/var/www If filesystem performance for builds or filesystem notifications for watches are what you need, check out the Filesystem Sync documentation to guide you through setup. Persistent Data Storage And finally we have a Persistent Data Volume, /data , that lives within the Docker Machine. Be sure to read about the Persistent Data Volume ( /data ) in the Key Concepts documentation. Often data may need to persist across container runs while needing some level of performance but not need to be shared directly with the host computer. Things like the /var/lib/mysql data directory are often in this category as you may want to export a DB to your host computer, but you very rarely want the raw MySQL data files. For these cases a volume mount from /data into your containers is what you need. For example: /data/project-one/local/mysql:/var/lib/mysql .outrigger.yml Finally, you can customize the rig project run command with common or project specific scripts to provide a considerable upgrade to the developer experience. See Project Configuration for details on how to use an .outrigger.yml file in your project directory to have project specific configuration integrated into rig .","title":"Existing Projects"},{"location":"project-setup/existing-projects/#existing-projects","text":"To add Outrigger support to an existing project, you'll need to understand how to create: Docker Compose file(s) Custom Domain Names for Project Services Local code sharing with your containers Persistent data storage An ( optional ) .outrigger.yml file","title":"Existing Projects"},{"location":"project-setup/existing-projects/#docker-compose-files","text":"Outrigger is intended to work with either raw Docker commands or Docker Compose files, and we find things are best when orchestrating your local environments using Docker Compose. Be sure to read the Docker Compose compose file documentation so that you have a handle on how to setup the needed configuration.","title":"Docker Compose file(s)"},{"location":"project-setup/existing-projects/#define-project-services","text":"The essential point is that you will need to define a Docker Compose Service for each of the components in your architecture. Web/API Server, database, cache/key-value store, search server, proxy cache, mail server, etc. Each of those will be configured as a service in your docker-compose.yml file. Please see the Outrigger Examples Repository to see setups of various technologies within Outrigger. Specifically, the Drupal 8 example covers all of the high points.","title":"Define Project Services"},{"location":"project-setup/existing-projects/#build-container","text":"In addition to containers for all of your project services, we find it useful to have a container we refer to as the build container. This container provides the tools supporting development of your project. The default build image for Outrigger is outrigger/build . It includes tools supporting a wide variety of modern web development such as node , npm , composer , grunt , gulp , bower , etc. We often configure the build container and associated command containers in a file named build.yml . The Drupal 8 example mentioned before contains a sample a build.yml file.","title":"Build Container"},{"location":"project-setup/existing-projects/#choosing-images-for-project-services","text":"Any valid Docker Image can work with Outrigger. We also supply a collection of crafted Docker images that are easily configured and optimized. An example for an Apache / PHP image based on PHP7 is outrigger/apache-php:php70 or php:7.1.1-apache Use Image Tags When selecting and image to use, be sure to also specify a tag for that image. When you do not specify a tag, the default tag of latest is used and it is easy to get out of sync with other team members or simply get unwanted changes if you do not explicitly specify an image tag.","title":"Choosing Images For Project Services"},{"location":"project-setup/existing-projects/#general-config","text":"Once you set up your docker-compose.yml file, you'll likely need to customize your services for a variety of reasons. Many images will use environment variables and/or volume mounts to customize the running container configuration. See Customizing Container Configuration to learn more about customizing your configuration for your project.","title":"General Config"},{"location":"project-setup/existing-projects/#custom-domain-names-for-project-services","text":"See DNS Resolution to learn how to use Docker Labels to specify custom domain names for your project's services.","title":"Custom Domain Names for Project Services"},{"location":"project-setup/existing-projects/#sharing-code-into-and-across-containers","text":"The straightforward way to get code from your host into containers is to use an NFS volume mount. This allows you, for example, to mount the current directory ( . ) to a specific place inside a container ( /var/www ) with a simple volume mount specification in your docker-compose.yml file. See the Volume Mount documentation on volume mount specifications. For example: .:/var/www If filesystem performance for builds or filesystem notifications for watches are what you need, check out the Filesystem Sync documentation to guide you through setup.","title":"Sharing code into and across containers"},{"location":"project-setup/existing-projects/#persistent-data-storage","text":"And finally we have a Persistent Data Volume, /data , that lives within the Docker Machine. Be sure to read about the Persistent Data Volume ( /data ) in the Key Concepts documentation. Often data may need to persist across container runs while needing some level of performance but not need to be shared directly with the host computer. Things like the /var/lib/mysql data directory are often in this category as you may want to export a DB to your host computer, but you very rarely want the raw MySQL data files. For these cases a volume mount from /data into your containers is what you need. For example: /data/project-one/local/mysql:/var/lib/mysql","title":"Persistent Data Storage"},{"location":"project-setup/existing-projects/#outriggeryml","text":"Finally, you can customize the rig project run command with common or project specific scripts to provide a considerable upgrade to the developer experience. See Project Configuration for details on how to use an .outrigger.yml file in your project directory to have project specific configuration integrated into rig .","title":".outrigger.yml"},{"location":"project-setup/filesystem-sync/","text":"Filesystem Sync Outrigger uses Unison to provide high-performance, bi-directional file synchronization between your local operating system and Docker containers. This is not the only means of sharing code or data with application containers, but it is the recommended approach for sharing files that must also be efficiently available to developers and their local tools (e.g., IDEs). With Unison, all the files from the root of your project are synced via a dedicated container and exposed as a named Docker Volume to the rest of your application. This volume is used as the file \"source\" for a volume mount to allow nearly native filesystem performance and features. For more on why Outrigger introduced Unison, jump down to Why Sync Instead of NFS? Setting Up Sync for Your Project's Containers To setup unison sync for your containers you will need to reference an external volume in your docker-compose.yml and use that external volume in the mount specification for any services that need to reference the same set of files. Since volumes are global within your Docker host, the name of this external volume should be namespaced to your project. The convention is to use projectname-sync as the volume name. Using the same volume will ensure all containers stay in sync. This includes specifying that volume for your build container , if you are using one (typically defined in build.yml). Add the volume to your docker-compose file Note that volume definitions are available as of docker-compose schema v2. volumes are a root-level property in the docker-compose schema and should not be nested below services. volumes: projectname-sync: external: true Note about volume name If you optionally specified a volume name to the project sync:start command or configured a name in your Outrigger Project Configuration file the name in this volumes section must match the one specified. The reason why we don't exclusively grab the volume name from the compose file is that this volume may be managed outside of Outrigger by another tool. We also want to support named volumes that don't precisely follow the *-sync convention described here. Use the external volume as your local mount point services: build: image: outrigger/build:php71 volumes: - projectname-sync:/var/www Managing the File Sync Process The file synchronization is managed by the rig cli via rig project sync:start and rig project sync:stop . Configuring the Sync Volume Name The name of the sync volume can be set in a variety of ways. It is determined using the following precedence: Specified in the sync - volume-name property of your Outrigger Project Configuration In your docker-compose.yml file, specified as an external volume with a name of the pattern *-sync If not specified anywhere else, it will use the name of the current project folder with -sync appended We recommend using approach #1 or #2 to ensure consistency across all environments. Approach #2 is most often used as it requires the most minimal configuration additions. If you use approach #1, make sure a volume of the same name is also defined as part of your docker or docker-compose configuration. Approach #1 is useful when the name of the volume can not follow the *-sync naming pattern for some reason. Configuring ignore When a need exists to restrict which files are synced using the ignore section. An example of limiting the synced files can be found in the sample in the Outrigger Project Configuration documentation. Start it up From your project root run the command rig project sync:start (there is also an alias rig project sync ) to get going. The initial sync can take a few seconds depending on the size of your project folder. You will see a progress indicator that will let you know when the initial sync is finished and things are ready to use. Workflow Change for Unison-using Projects Projects that use this file syncing approach will require execution of this command as a new step before any other docker commands can be run. Cleaning up Since the project sync:start command starts another container to manage the volume syncing, we have a command project sync:stop that will clean up that running container for you. It discovers the volume / container name the same way project sync:start does. When you are done for the day, or for that project, run rig project sync:stop from the project root to clean up any running sync containers for that project. Additional Sync Commands As of version 2.2.0, Outrigger's project sync has commands as detailed below to facilitate scripting and troubleshooting. sync:check The project sync:check command performs tests to verify if the sync process is working by looking to see if the Unison container is running, can accept connections and a file successfully transfers. sync:name The project sync:name command outputs the name of the docker container and volume that files are being synchronized with. It performs the same lookup logic as project sync:start does but only outputs the name of the volume and container to facilitate scripting processes that need it. sync:purge The project sync:purge command removes any running sync process for the project and deletes the docker volume being synced with. It also cleans up any temporary files created by Unison. How File Sync works in Linux Environments Linux environments have native filesystem performance without anything as complex as file sync, because they do not need to use a virtualization layer to have access to the Linux Kernel's container support . For this reason, any environment running on Linux will have local bind volumes created automatically where a Unison/sync volume would be used. This allows for the use of named volumes, but avoids using Unison where local filesystems have all the functionality needed. Setting up file sync manually using Override Files See Understanding multiple Compose files to understand how Docker Compose files work. Example docker-compose.yml services: www: image: outrigger/apache-php:php71 volumes: - .:/var/www Example docker-compose.override.yml services: www: volumes: - projectname-sync:/var/www volumes: projectname-sync: external: true Why Sync Instead of NFS? Outrigger provides easy, NFS-based filesystem mounts to share code and files with your project containers. However, it is slow when writing, reading, or scanning thousands of files when compared to native filesystem performance. Some modern tool kits and package managers favor large numbers of small files and libraries so maintaining high performance of a build becomes challenging when your code base is on a volume mounted NFS share. NFS also has the downside of not propagating filesystem modification events. This can be problematic if you want to run a process in your container to rebuild or update compiled project assets when modification notifications are triggered due to local file editing in your IDE. Given the current state of the art of Virtual Machines and filesystems, these types of operations are better supported by bi-directional file syncing (which includes filesystem notifications).","title":"Filesystem Sync"},{"location":"project-setup/filesystem-sync/#filesystem-sync","text":"Outrigger uses Unison to provide high-performance, bi-directional file synchronization between your local operating system and Docker containers. This is not the only means of sharing code or data with application containers, but it is the recommended approach for sharing files that must also be efficiently available to developers and their local tools (e.g., IDEs). With Unison, all the files from the root of your project are synced via a dedicated container and exposed as a named Docker Volume to the rest of your application. This volume is used as the file \"source\" for a volume mount to allow nearly native filesystem performance and features. For more on why Outrigger introduced Unison, jump down to Why Sync Instead of NFS?","title":"Filesystem Sync"},{"location":"project-setup/filesystem-sync/#setting-up-sync-for-your-projects-containers","text":"To setup unison sync for your containers you will need to reference an external volume in your docker-compose.yml and use that external volume in the mount specification for any services that need to reference the same set of files. Since volumes are global within your Docker host, the name of this external volume should be namespaced to your project. The convention is to use projectname-sync as the volume name. Using the same volume will ensure all containers stay in sync. This includes specifying that volume for your build container , if you are using one (typically defined in build.yml).","title":"Setting Up Sync for Your Project's Containers"},{"location":"project-setup/filesystem-sync/#add-the-volume-to-your-docker-compose-file","text":"Note that volume definitions are available as of docker-compose schema v2. volumes are a root-level property in the docker-compose schema and should not be nested below services. volumes: projectname-sync: external: true Note about volume name If you optionally specified a volume name to the project sync:start command or configured a name in your Outrigger Project Configuration file the name in this volumes section must match the one specified. The reason why we don't exclusively grab the volume name from the compose file is that this volume may be managed outside of Outrigger by another tool. We also want to support named volumes that don't precisely follow the *-sync convention described here.","title":"Add the volume to your docker-compose file"},{"location":"project-setup/filesystem-sync/#use-the-external-volume-as-your-local-mount-point","text":"services: build: image: outrigger/build:php71 volumes: - projectname-sync:/var/www","title":"Use the external volume as your local mount point"},{"location":"project-setup/filesystem-sync/#managing-the-file-sync-process","text":"The file synchronization is managed by the rig cli via rig project sync:start and rig project sync:stop .","title":"Managing the File Sync Process"},{"location":"project-setup/filesystem-sync/#configuring-the-sync-volume-name","text":"The name of the sync volume can be set in a variety of ways. It is determined using the following precedence: Specified in the sync - volume-name property of your Outrigger Project Configuration In your docker-compose.yml file, specified as an external volume with a name of the pattern *-sync If not specified anywhere else, it will use the name of the current project folder with -sync appended We recommend using approach #1 or #2 to ensure consistency across all environments. Approach #2 is most often used as it requires the most minimal configuration additions. If you use approach #1, make sure a volume of the same name is also defined as part of your docker or docker-compose configuration. Approach #1 is useful when the name of the volume can not follow the *-sync naming pattern for some reason.","title":"Configuring the Sync Volume Name"},{"location":"project-setup/filesystem-sync/#configuring-ignore","text":"When a need exists to restrict which files are synced using the ignore section. An example of limiting the synced files can be found in the sample in the Outrigger Project Configuration documentation.","title":"Configuring ignore"},{"location":"project-setup/filesystem-sync/#start-it-up","text":"From your project root run the command rig project sync:start (there is also an alias rig project sync ) to get going. The initial sync can take a few seconds depending on the size of your project folder. You will see a progress indicator that will let you know when the initial sync is finished and things are ready to use. Workflow Change for Unison-using Projects Projects that use this file syncing approach will require execution of this command as a new step before any other docker commands can be run.","title":"Start it up"},{"location":"project-setup/filesystem-sync/#cleaning-up","text":"Since the project sync:start command starts another container to manage the volume syncing, we have a command project sync:stop that will clean up that running container for you. It discovers the volume / container name the same way project sync:start does. When you are done for the day, or for that project, run rig project sync:stop from the project root to clean up any running sync containers for that project.","title":"Cleaning up"},{"location":"project-setup/filesystem-sync/#additional-sync-commands","text":"As of version 2.2.0, Outrigger's project sync has commands as detailed below to facilitate scripting and troubleshooting.","title":"Additional Sync Commands"},{"location":"project-setup/filesystem-sync/#synccheck","text":"The project sync:check command performs tests to verify if the sync process is working by looking to see if the Unison container is running, can accept connections and a file successfully transfers.","title":"sync:check"},{"location":"project-setup/filesystem-sync/#syncname","text":"The project sync:name command outputs the name of the docker container and volume that files are being synchronized with. It performs the same lookup logic as project sync:start does but only outputs the name of the volume and container to facilitate scripting processes that need it.","title":"sync:name"},{"location":"project-setup/filesystem-sync/#syncpurge","text":"The project sync:purge command removes any running sync process for the project and deletes the docker volume being synced with. It also cleans up any temporary files created by Unison.","title":"sync:purge"},{"location":"project-setup/filesystem-sync/#how-file-sync-works-in-linux-environments","text":"Linux environments have native filesystem performance without anything as complex as file sync, because they do not need to use a virtualization layer to have access to the Linux Kernel's container support . For this reason, any environment running on Linux will have local bind volumes created automatically where a Unison/sync volume would be used. This allows for the use of named volumes, but avoids using Unison where local filesystems have all the functionality needed.","title":"How File Sync works in Linux Environments"},{"location":"project-setup/filesystem-sync/#setting-up-file-sync-manually-using-override-files","text":"See Understanding multiple Compose files to understand how Docker Compose files work.","title":"Setting up file sync manually using Override Files"},{"location":"project-setup/filesystem-sync/#example-docker-composeyml","text":"services: www: image: outrigger/apache-php:php71 volumes: - .:/var/www","title":"Example docker-compose.yml"},{"location":"project-setup/filesystem-sync/#example-docker-composeoverrideyml","text":"services: www: volumes: - projectname-sync:/var/www volumes: projectname-sync: external: true","title":"Example docker-compose.override.yml"},{"location":"project-setup/filesystem-sync/#why-sync-instead-of-nfs","text":"Outrigger provides easy, NFS-based filesystem mounts to share code and files with your project containers. However, it is slow when writing, reading, or scanning thousands of files when compared to native filesystem performance. Some modern tool kits and package managers favor large numbers of small files and libraries so maintaining high performance of a build becomes challenging when your code base is on a volume mounted NFS share. NFS also has the downside of not propagating filesystem modification events. This can be problematic if you want to run a process in your container to rebuild or update compiled project assets when modification notifications are triggered due to local file editing in your IDE. Given the current state of the art of Virtual Machines and filesystems, these types of operations are better supported by bi-directional file syncing (which includes filesystem notifications).","title":"Why Sync Instead of NFS?"},{"location":"project-setup/key-concepts/","text":"Common Setup Before you begin there are two aspects of Docker and Outrigger setup that you must be aware of. These relate to how you share your code with your running Docker containers and how you ensure that any data you need to persist between executions of your containers is preserved. The examples you see throughout this documentation demonstrate configurations that take these aspects into consideration. Project Code The filesystem within containers is ephemeral! Any changes made there do not persist if the container is restarted. (See \"Persistent Data Volume\" below for a storage area that is preserved.) In typical Docker images the code is built directly into the container at the /code path. This is a great mechanism that allows the container to be \"self contained\" (pun intended), immutable and not need any checkout/file system. You also know when you run a container exactly what code is in there because you generally don\u2019t change the code unless you rebuild the image. For development purposes, however, this is problematic because it is burdensome to rebuild an image for each code change. To solve this, there are a few available approaches you can take. Approach 1: NFS Filesystem Outrigger sets up an NFS filesystem from your Host Machine to the Docker host that you can leverage. Using this, you can easily mount your project code from the Host Machine into the running container, effectively overriding the files built directly into the image. The running container is then using the local project file system for the overridden paths rather than the file system built into the image. This allows a developer to use an IDE and edit code directly on the local file system of the Host Machine, but execute that code within the environment of the running container. Note on Project Code Location Your project code must be located somewhere within your home directory ( /Users for Macs) on your local machine. This is because NFS shares your home directory into the Docker Host VM, and only files on the Docker Host VM can be referenced in volume shares. Approach 2: Docker Volumes and Unison There are some drawbacks to using the NFS Filesystem so your second option is to use a Data Volume to hold a mirror of your code base and user Outrigger's Filesystem Sync capabilities to automatically keep it in sync with your local code base. There are several advantages to this approach detailed on the filesystem sync page at the cost of a slightly more complex setup. Outrigger helps manage the creation and syncing of the data volume to minimize this complexity. Persistent Data Volume Outrigger maintains a data volume on the Docker Host that is mounted at /data into every container. This volume is persistent so long as you do not perform a rig remove operation. This ensures that file access on the VM is done natively for things like databases and other things where filesystem performance matters. If you configure a container to write to this area, you should use a project and container based namespace to prevent conflicts as this is a shared resource. For example, you may want to use a namespacing method like /data/[project name]/[environment]/[service name] as a safe location to write data. Over time you may find it necessary to manage this directory and delete old project data. This can be done through use of docker-machine ssh dev and then direct manipulation of /data . Any code from your local directories is directly shared in to the Docker Machine VM via NFS at /Users . This means that even if you destroy and re-create your Docker Host, your code will be safe since it lives on the Host Machine. FILE CHANGES WITHIN A CONTAINER Any files that are generated or changed within a running container that you want to persist after the container is stopped should be put onto a persistent volume local machine . A Docker container represents immutable infrastructure, the files on the image are able to be changed at runtime but typically do not persist. When the container is stopped and run again it \"boots\" the files that were built into the original image. The volume at /data (mentioned above) is a persistent volume that you can use and Docker Data Volumes represent another. Outrigger helps maintain the contents of the /data directory between upgrades of your Docker Host Virtual Machine though occasional maintenance to remove old project data will be needed. Docker data volumes do not persist between virtual machine upgrades but may be easier to manage using docker volume commands.","title":"Key Concepts"},{"location":"project-setup/key-concepts/#common-setup","text":"Before you begin there are two aspects of Docker and Outrigger setup that you must be aware of. These relate to how you share your code with your running Docker containers and how you ensure that any data you need to persist between executions of your containers is preserved. The examples you see throughout this documentation demonstrate configurations that take these aspects into consideration.","title":"Common Setup"},{"location":"project-setup/key-concepts/#project-code","text":"The filesystem within containers is ephemeral! Any changes made there do not persist if the container is restarted. (See \"Persistent Data Volume\" below for a storage area that is preserved.) In typical Docker images the code is built directly into the container at the /code path. This is a great mechanism that allows the container to be \"self contained\" (pun intended), immutable and not need any checkout/file system. You also know when you run a container exactly what code is in there because you generally don\u2019t change the code unless you rebuild the image. For development purposes, however, this is problematic because it is burdensome to rebuild an image for each code change. To solve this, there are a few available approaches you can take.","title":"Project Code"},{"location":"project-setup/key-concepts/#approach-1-nfs-filesystem","text":"Outrigger sets up an NFS filesystem from your Host Machine to the Docker host that you can leverage. Using this, you can easily mount your project code from the Host Machine into the running container, effectively overriding the files built directly into the image. The running container is then using the local project file system for the overridden paths rather than the file system built into the image. This allows a developer to use an IDE and edit code directly on the local file system of the Host Machine, but execute that code within the environment of the running container. Note on Project Code Location Your project code must be located somewhere within your home directory ( /Users for Macs) on your local machine. This is because NFS shares your home directory into the Docker Host VM, and only files on the Docker Host VM can be referenced in volume shares.","title":"Approach 1: NFS Filesystem"},{"location":"project-setup/key-concepts/#approach-2-docker-volumes-and-unison","text":"There are some drawbacks to using the NFS Filesystem so your second option is to use a Data Volume to hold a mirror of your code base and user Outrigger's Filesystem Sync capabilities to automatically keep it in sync with your local code base. There are several advantages to this approach detailed on the filesystem sync page at the cost of a slightly more complex setup. Outrigger helps manage the creation and syncing of the data volume to minimize this complexity.","title":"Approach 2: Docker Volumes and Unison"},{"location":"project-setup/key-concepts/#persistent-data-volume","text":"Outrigger maintains a data volume on the Docker Host that is mounted at /data into every container. This volume is persistent so long as you do not perform a rig remove operation. This ensures that file access on the VM is done natively for things like databases and other things where filesystem performance matters. If you configure a container to write to this area, you should use a project and container based namespace to prevent conflicts as this is a shared resource. For example, you may want to use a namespacing method like /data/[project name]/[environment]/[service name] as a safe location to write data. Over time you may find it necessary to manage this directory and delete old project data. This can be done through use of docker-machine ssh dev and then direct manipulation of /data . Any code from your local directories is directly shared in to the Docker Machine VM via NFS at /Users . This means that even if you destroy and re-create your Docker Host, your code will be safe since it lives on the Host Machine. FILE CHANGES WITHIN A CONTAINER Any files that are generated or changed within a running container that you want to persist after the container is stopped should be put onto a persistent volume local machine . A Docker container represents immutable infrastructure, the files on the image are able to be changed at runtime but typically do not persist. When the container is stopped and run again it \"boots\" the files that were built into the original image. The volume at /data (mentioned above) is a persistent volume that you can use and Docker Data Volumes represent another. Outrigger helps maintain the contents of the /data directory between upgrades of your Docker Host Virtual Machine though occasional maintenance to remove old project data will be needed. Docker data volumes do not persist between virtual machine upgrades but may be easier to manage using docker volume commands.","title":"Persistent Data Volume"},{"location":"project-setup/new-projects/","text":"New Projects To setup new projects in Outrigger use the command rig project create to coordinate all of the various operations required to get the proper configuration in place. The project create command runs a collection of Yeoman generators, prompting for input and outputting a collection of Docker Compose and Outrigger configuration. Project Create command The project create command has the following usage: rig project create [--image image:tag] [type] [args] Running the project create command without specifying the --image flag will run using the Outrigger Generator image . Additionally, if no [type] is specified the default type is outrigger-drupal . Other options for [type] on the default outrigger/generator image are gadget and pattern-lab-starter . Documentation for the currently supported types and related args can be found here: outrigger-drupal gadget patern-lab-starter Specify a Generator Image If you want to use a different image other than outrigger/generator:latest you can specify it with the --image flag. Generator images should expect to output their work at path /generated within the running container. The project create command mounts the current working directory at this path in order to capture generated output to your host machine. All [type] and [arg] options are passed to the docker run command launching the container for the specified image.","title":"New Projects"},{"location":"project-setup/new-projects/#new-projects","text":"To setup new projects in Outrigger use the command rig project create to coordinate all of the various operations required to get the proper configuration in place. The project create command runs a collection of Yeoman generators, prompting for input and outputting a collection of Docker Compose and Outrigger configuration.","title":"New Projects"},{"location":"project-setup/new-projects/#project-create-command","text":"The project create command has the following usage: rig project create [--image image:tag] [type] [args] Running the project create command without specifying the --image flag will run using the Outrigger Generator image . Additionally, if no [type] is specified the default type is outrigger-drupal . Other options for [type] on the default outrigger/generator image are gadget and pattern-lab-starter . Documentation for the currently supported types and related args can be found here: outrigger-drupal gadget patern-lab-starter Specify a Generator Image If you want to use a different image other than outrigger/generator:latest you can specify it with the --image flag. Generator images should expect to output their work at path /generated within the running container. The project create command mounts the current working directory at this path in order to capture generated output to your host machine. All [type] and [arg] options are passed to the docker run command launching the container for the specified image.","title":"Project Create command"},{"location":"project-setup/project-configuration/","text":"Project Configuration Projects can have a local outrigger.yml file included in their project to define the rig project commands. This can be done to streamline the developer experience of the project, or to capture project specifics / intricacies in a place that is consistent for all team members. File Location The configuration file is typically named outrigger.yml and is normally placed at the root of a project directory tree. You may specify a $RIG_PROJECT_CONFIG_FILE environment variable to override this which can be useful for running rig project commands from different directories. Features Project Scripts Much like composer and npm, rig project supports the creation of project-standardized scripts in this configuration file. Scripts are executed relative to the location of the configuration file. The bin property is prepended to your $PATH variable, simplifying the script configuration. Each \"script\" may be made up of a series of steps, if any step fails remaining steps will not be executed. When you run a script (such as rig project run:up ) you can include additional parameters. Such additonal parameters will be appended to the final step of the script. Commands are defined in rig project based on the script ID prefixed by run: , so a command such as up is primarily available as rig project run:up . Any script may specify a single alias to shorten this. An alias such as 'up' would change this to rig project up . Configure Filesystem Sync As described in Filesystem Sync , rig project brokers the synchronization process. You can tailor this process to specify the name of the Docker volume or set files or directories to be skipped by this process as a performance optimization. None of this configuration is required. Configuration File Below is a sample configuration file with comments to describe the various configuration options. # Required version so we can ensure compatibility version: 1.0 # This is prepended to the $PATH for any commands referenced in the scripts section. bin: ./bin:./node_modules/.bin:./vendor/bin # Project Scripts # These can be run via 'rig project run: key ' # If you specify an alias, you can run 'rig project alias ' scripts: # This will be `rig project run:up` up: # Or with an alias, `rig project up` alias: up # Description is used for help when running `rig project help` or `rig project run help` description: Start up operational docker containers and filesystem sync. # These are the various run steps, they will be concatenated together into a single command with ' ' run: - rig project sync:start - docker-compose up -d stop: alias: down description: Halt operational containers and filesystem sync. run: - docker-compose stop - rig project sync:stop # This controls configuration for the `project sync:start` command. sync: # This is the name of the external volume to use (optional). Needs to match volume name in Docker Compose. volume: project-sync # These ignores will be added to unison and not synced between local and project volume ignore: - Name .git - Path build/tmp - Regex full/path/to/*.json See this Outrigger Project configuration file , from the outrigger-drupal generator, for an example of a detailed outrigger.yml file. See unison ignoring , for more ignoring examples.","title":"Project Configuration"},{"location":"project-setup/project-configuration/#project-configuration","text":"Projects can have a local outrigger.yml file included in their project to define the rig project commands. This can be done to streamline the developer experience of the project, or to capture project specifics / intricacies in a place that is consistent for all team members.","title":"Project Configuration"},{"location":"project-setup/project-configuration/#file-location","text":"The configuration file is typically named outrigger.yml and is normally placed at the root of a project directory tree. You may specify a $RIG_PROJECT_CONFIG_FILE environment variable to override this which can be useful for running rig project commands from different directories.","title":"File Location"},{"location":"project-setup/project-configuration/#features","text":"","title":"Features"},{"location":"project-setup/project-configuration/#project-scripts","text":"Much like composer and npm, rig project supports the creation of project-standardized scripts in this configuration file. Scripts are executed relative to the location of the configuration file. The bin property is prepended to your $PATH variable, simplifying the script configuration. Each \"script\" may be made up of a series of steps, if any step fails remaining steps will not be executed. When you run a script (such as rig project run:up ) you can include additional parameters. Such additonal parameters will be appended to the final step of the script. Commands are defined in rig project based on the script ID prefixed by run: , so a command such as up is primarily available as rig project run:up . Any script may specify a single alias to shorten this. An alias such as 'up' would change this to rig project up .","title":"Project Scripts"},{"location":"project-setup/project-configuration/#configure-filesystem-sync","text":"As described in Filesystem Sync , rig project brokers the synchronization process. You can tailor this process to specify the name of the Docker volume or set files or directories to be skipped by this process as a performance optimization. None of this configuration is required.","title":"Configure Filesystem Sync"},{"location":"project-setup/project-configuration/#configuration-file","text":"Below is a sample configuration file with comments to describe the various configuration options. # Required version so we can ensure compatibility version: 1.0 # This is prepended to the $PATH for any commands referenced in the scripts section. bin: ./bin:./node_modules/.bin:./vendor/bin # Project Scripts # These can be run via 'rig project run: key ' # If you specify an alias, you can run 'rig project alias ' scripts: # This will be `rig project run:up` up: # Or with an alias, `rig project up` alias: up # Description is used for help when running `rig project help` or `rig project run help` description: Start up operational docker containers and filesystem sync. # These are the various run steps, they will be concatenated together into a single command with ' ' run: - rig project sync:start - docker-compose up -d stop: alias: down description: Halt operational containers and filesystem sync. run: - docker-compose stop - rig project sync:stop # This controls configuration for the `project sync:start` command. sync: # This is the name of the external volume to use (optional). Needs to match volume name in Docker Compose. volume: project-sync # These ignores will be added to unison and not synced between local and project volume ignore: - Name .git - Path build/tmp - Regex full/path/to/*.json See this Outrigger Project configuration file , from the outrigger-drupal generator, for an example of a detailed outrigger.yml file. See unison ignoring , for more ignoring examples.","title":"Configuration File"}]}